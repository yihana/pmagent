# server/workflow/agents/quality_agent.py

from typing import Any, Dict, List, Optional
import json
import logging

logger = logging.getLogger(__name__)

class QualityAgent:
    """ìš”êµ¬ì‚¬í•­ í’ˆì§ˆ ê²€ì¦ Agent"""
    
    def __init__(self, llm=None, threshold: float = 75.0):
        from server.utils.config import get_llm
        self.llm = llm or get_llm()
        self.threshold = threshold
    
    def validate(self, 
                 requirements: List[Dict[str, Any]], 
                 original_text: str,
                 metadata: Optional[Dict] = None) -> Dict[str, Any]:
        """
        ìš”êµ¬ì‚¬í•­ í’ˆì§ˆ ê²€ì¦
        
        Returns:
            {
                "pass": bool,
                "score": float,
                "grade": str,
                "metrics": {...},
                "issues": [...],
                "recommendations": [...]
            }
        """
        logger.info(f"ğŸ” í’ˆì§ˆ ê²€ì¦ ì‹œì‘ - ìš”êµ¬ì‚¬í•­ {len(requirements)}ê°œ")
        
        # 1. êµ¬ì¡°ì  ê²€ì¦ (Rule-based)
        structure_result = self._validate_structure(requirements)
        
        # 2. ì˜ë¯¸ì  ê²€ì¦ (LLM-based)
        semantic_result = self._validate_semantics(
            requirements, 
            original_text
        )
        
        # 3. ì¢…í•© íŒì •
        result = self._aggregate_results(
            structure_result, 
            semantic_result
        )
        
        logger.info(f"âœ… ê²€ì¦ ì™„ë£Œ - ì ìˆ˜: {result['score']}, ë“±ê¸‰: {result['grade']}")
        
        return result
    
    def _validate_structure(self, requirements: List[Dict]) -> Dict:
        """êµ¬ì¡°ì  ê²€ì¦"""
        score = 0
        max_score = 30
        issues = []
        
        logger.debug("êµ¬ì¡°ì  ê²€ì¦ ì‹œì‘")
        
        # í•„ìˆ˜ í•„ë“œ ì²´í¬
        required_fields = ['req_id', 'title', 'description', 'type', 
                          'priority', 'source_span']
        
        for req in requirements:
            req_id = req.get('req_id', 'Unknown')
            
            # 1. í•„ìˆ˜ í•„ë“œ ì¡´ì¬ (10ì )
            missing = [f for f in required_fields if f not in req]
            if not missing:
                score += 10 / len(requirements)
            else:
                issues.append(f"{req_id}: í•„ë“œ ëˆ„ë½ - {missing}")
            
            # 2. acceptance_criteria í’ˆì§ˆ (10ì )
            ac = req.get('acceptance_criteria', [])
            if len(ac) >= 2:
                score += 5 / len(requirements)
            else:
                issues.append(f"{req_id}: acceptance_criteria ë¶€ì¡± ({len(ac)}ê°œ)")
            
            if ac and all(len(str(c)) > 15 for c in ac):
                score += 5 / len(requirements)
            
            # 3. description ê¸¸ì´ (5ì )
            desc = req.get('description', '')
            if len(desc) > 30:
                score += 5 / len(requirements)
            else:
                issues.append(f"{req_id}: description ë„ˆë¬´ ì§§ìŒ")
        
        # 4. íƒ€ì… ë¶„í¬ (5ì )
        types = [r.get('type') for r in requirements]
        if 'functional' in types and 'non-functional' in types:
            score += 5
        
        return {
            "score": min(score, max_score),
            "max_score": max_score,
            "issues": issues
        }
    
    def _validate_semantics(self, 
                           requirements: List[Dict], 
                           original_text: str) -> Dict:
        """ì˜ë¯¸ì  ê²€ì¦ (LLM ì‚¬ìš©)"""
        logger.debug("ì˜ë¯¸ì  ê²€ì¦ ì‹œì‘")
        
        prompt = self._build_validation_prompt(requirements, original_text)
        
        try:
            messages = [
                {"role": "system", "content": "You are a requirements quality expert."},
                {"role": "user", "content": prompt}
            ]
            response = self.llm.invoke(messages)
            
            # ì‘ë‹µì—ì„œ content ì¶”ì¶œ
            if hasattr(response, 'content'):
                content = response.content
            else:
                content = str(response)
            
            # JSON íŒŒì‹±
            result = self._parse_llm_response(content)
            
            logger.info(f"LLM ê²€ì¦ ì ìˆ˜: {result.get('total_score', 0)}")
            
            return result
            
        except Exception as e:
            logger.error(f"LLM ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {
                "score": 0,
                "max_score": 70,
                "issues": [f"LLM ê²€ì¦ ì‹¤íŒ¨: {e}"]
            }
    
    def _build_validation_prompt(self, 
                                 requirements: List[Dict], 
                                 original_text: str) -> str:
        """ê²€ì¦ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        reqs_summary = []
        for r in requirements[:10]:  # ìµœëŒ€ 10ê°œë§Œ
            reqs_summary.append({
                "req_id": r.get('req_id'),
                "title": r.get('title'),
                "type": r.get('type')
            })
        
        return f"""
ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ ê²°ê³¼ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì„¸ìš”.

## ì›ë¬¸ (ì²˜ìŒ 2000ì)
{original_text[:2000]}

## ì¶”ì¶œëœ ìš”êµ¬ì‚¬í•­ ({len(requirements)}ê°œ)
{json.dumps(reqs_summary, ensure_ascii=False, indent=2)}

## í‰ê°€ ê¸°ì¤€

### 1. Completeness (ì™„ì„±ë„) - 30ì 
- ì›ë¬¸ì˜ ì£¼ìš” ë‚´ìš©ì´ ë¹ ì§ì—†ì´ ì¶”ì¶œë˜ì—ˆëŠ”ê°€?
- ì¤‘ìš”í•œ ìš”êµ¬ì‚¬í•­ì´ ëˆ„ë½ë˜ì§€ ì•Šì•˜ëŠ”ê°€?

### 2. Clarity (ëª…í™•ì„±) - 25ì 
- ê° ìš”êµ¬ì‚¬í•­ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?
- ëª¨í˜¸í•œ í‘œí˜„ì´ ì—†ëŠ”ê°€?

### 3. Consistency (ì¼ê´€ì„±) - 15ì 
- ì¤‘ë³µëœ ìš”êµ¬ì‚¬í•­ì´ ì—†ëŠ”ê°€?
- ëª¨ìˆœë˜ëŠ” ë‚´ìš©ì´ ì—†ëŠ”ê°€?

## ì¶œë ¥ í˜•ì‹ (JSONë§Œ)

{{{{
  "completeness_score": 0-30,
  "completeness_issues": ["ì´ìŠˆ1", "ì´ìŠˆ2"],
  
  "clarity_score": 0-25,
  "clarity_issues": ["ì´ìŠˆ1"],
  
  "consistency_score": 0-15,
  "consistency_issues": ["ì´ìŠˆ1"],
  
  "total_score": 0-70,
  
  "missing_requirements": [
    "ëˆ„ë½ëœ ê²ƒìœ¼ë¡œ ë³´ì´ëŠ” ìš”êµ¬ì‚¬í•­"
  ],
  
  "recommendations": [
    "REQ-001ì˜ descriptionì„ ë” êµ¬ì²´í™”í•˜ì„¸ìš”",
    "REQ-003ê³¼ REQ-004ê°€ ìœ ì‚¬í•˜ì—¬ ë³‘í•© ê²€í†  í•„ìš”"
  ]
}}}}

JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.
"""
    
    def _parse_llm_response(self, content: str) -> Dict:
        """LLM ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON ì¶”ì¶œ
            import re
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
                return {
                    "score": data.get('total_score', 0),
                    "max_score": 70,
                    "completeness": data.get('completeness_score', 0),
                    "clarity": data.get('clarity_score', 0),
                    "consistency": data.get('consistency_score', 0),
                    "issues": (data.get('completeness_issues', []) +
                              data.get('clarity_issues', []) +
                              data.get('consistency_issues', [])),
                    "missing": data.get('missing_requirements', []),
                    "recommendations": data.get('recommendations', [])
                }
        except Exception as e:
            logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        return {"score": 0, "max_score": 70, "issues": ["íŒŒì‹± ì‹¤íŒ¨"]}
    
    def _aggregate_results(self, 
                          structure: Dict, 
                          semantic: Dict) -> Dict:
        """ê²°ê³¼ í†µí•©"""
        
        total_score = structure['score'] + semantic['score']
        max_score = structure['max_score'] + semantic['max_score']
        percentage = (total_score / max_score * 100) if max_score > 0 else 0
        
        # ë“±ê¸‰ íŒì •
        if percentage >= 90:
            grade = "Excellent"
            pass_status = True
            action = "accept"
        elif percentage >= 75:
            grade = "Good"
            pass_status = True
            action = "accept_with_warning"
        elif percentage >= 60:
            grade = "Fair"
            pass_status = False
            action = "refinement_required"
        else:
            grade = "Poor"
            pass_status = False
            action = "reject_and_retry"
        
        all_issues = structure['issues'] + semantic.get('issues', [])
        
        return {
            "pass": pass_status,
            "score": round(percentage, 1),
            "grade": grade,
            "action": action,
            "metrics": {
                "structure_score": structure['score'],
                "semantic_score": semantic['score'],
                "total_score": total_score,
                "max_score": max_score
            },
            "issues": all_issues,
            "missing_requirements": semantic.get('missing', []),
            "recommendations": semantic.get('recommendations', [])
        }


# ê²€ì¦ ë£¨í”„ í†µí•©
def extract_with_validation(scope_agent, quality_agent, 
                            text: str, 
                            max_attempts: int = 3,
                            threshold: float = 75.0):
    """ê²€ì¦ì„ í¬í•¨í•œ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤"""
    
    for attempt in range(1, max_attempts + 1):
        logger.info(f"ğŸ“ ì¶”ì¶œ ì‹œë„ #{attempt}")
        
        # 1. ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
        if attempt == 1:
            result = scope_agent.extract_requirements(text)
        else:
            # ì´ì „ ê²€ì¦ ê²°ê³¼ë¥¼ ë°˜ì˜í•˜ì—¬ ì¬ì¶”ì¶œ
            result = scope_agent.refine_requirements(
                text, 
                previous_result, 
                validation_result
            )
        
        requirements = result.get('requirements', [])
        logger.info(f"âœ… {len(requirements)}ê°œ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ")
        
        # 2. í’ˆì§ˆ ê²€ì¦
        validation_result = quality_agent.validate(requirements, text)
        
        logger.info(f"ğŸ¯ í’ˆì§ˆ ì ìˆ˜: {validation_result['score']} "
                   f"({validation_result['grade']})")
        
        # 3. í†µê³¼ ì—¬ë¶€ í™•ì¸
        if validation_result['pass']:
            logger.info(f"âœ… ê²€ì¦ í†µê³¼! (ì‹œë„ {attempt}íšŒ)")
            return {
                "success": True,
                "requirements": requirements,
                "validation": validation_result,
                "attempts": attempt
            }
        
        logger.warning(f"âš ï¸ ê²€ì¦ ì‹¤íŒ¨. ì¬ì‹œë„ í•„ìš”")
        logger.warning(f"ì£¼ìš” ì´ìŠˆ: {validation_result['issues'][:3]}")
        
        previous_result = result
    
    # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
    logger.error(f"âŒ {max_attempts}íšŒ ì‹œë„ í›„ì—ë„ ê²€ì¦ ì‹¤íŒ¨")
    return {
        "success": False,
        "requirements": requirements,
        "validation": validation_result,
        "attempts": max_attempts
    }