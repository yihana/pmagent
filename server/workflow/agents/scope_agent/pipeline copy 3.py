from __future__ import annotations
import os, re, json, asyncio, time, logging, traceback
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

from server.workflow.agents.scope_agent.prompts import build_scope_prompt
from server.workflow.agents.scope_agent.prompts import (
    PROJECT_CHARTER_PROMPT, TAILORING_PROMPT
)

from server.workflow.agents.scope_agent.outputs.project_charter import ProjectCharterGenerator
from server.workflow.agents.scope_agent.outputs.scope_statement import ScopeStatementGenerator
from server.workflow.agents.scope_agent.outputs.rtm_excel import RTMExcelGenerator
from server.workflow.agents.scope_agent.outputs.wbs_excel import WBSExcelGenerator
from server.workflow.agents.scope_agent.outputs.tailoring import TailoringGenerator
from server.workflow.agents.scope_agent.outputs.project_plan import ProjectPlanGenerator  # ì‹ ê·œ ì—°ê²°


logger = logging.getLogger("scope.agent")

# DB imports (optional)
try:
    from server.db.database import SessionLocal
    from server.db import pm_models
    _DB_AVAILABLE = True
except Exception as e:
    logger.warning("[ScopeAgent] DB import failed: %s", e)
    SessionLocal = None
    pm_models = None
    _DB_AVAILABLE = False

# --- LangChain v0.2+ í˜¸í™˜ import (fallback í¬í•¨) ---
try:
    from langchain_openai import OpenAIEmbeddings, AzureOpenAIEmbeddings
    from langchain_community.vectorstores import FAISS
except Exception:
    # êµ¬ë²„ì „ í˜¸í™˜
    from langchain.embeddings import OpenAIEmbeddings  # type: ignore
    from langchain.vectorstores import FAISS  # type: ignore

# ---------------------------------------------------------------------
# LLM getter
# ---------------------------------------------------------------------
def get_llm():
    try:
        from server.utils.config import get_llm as _g
        llm = _g()
        logger.debug("[SCOPE_AGENT] get_llm() success: %s", getattr(llm, "__class__", llm))
        return llm
    except Exception as e:
        logger.warning("[SCOPE_AGENT] get_llm failed: %s", e)
        return None

# ---------------------------------------------------------------------
# ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
# ---------------------------------------------------------------------
def _safe_extract_raw(resp: Any) -> str:
    """LLM ì‘ë‹µì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ì¶”ì¶œ(ìœ ì—° ì²˜ë¦¬)."""
    try:
        if resp is None:
            return ""
        
        # ì´ë¯¸ ë¬¸ìì—´ì¸ ê²½ìš° ë°”ë¡œ ë°˜í™˜ (ê°€ì¥ í”í•œ ì¼€ì´ìŠ¤)
        if isinstance(resp, str):
            logger.debug("[SCOPE] LLM ì‘ë‹µì´ ì´ë¯¸ ë¬¸ìì—´ì…ë‹ˆë‹¤")
            return resp
        
        # langchain/chat model-like: resp.generations / resp.generations[0].message.content
        if hasattr(resp, "generations"):
            gens = getattr(resp, "generations")
            try:
                # try to flatten common shapes
                if isinstance(gens, list) and len(gens) and hasattr(gens[0], "message"):
                    content = gens[0].message.content
                    logger.debug("[SCOPE] LLM ì‘ë‹µ ì¶”ì¶œ: generations[0].message.content")
                    return content
            except Exception:
                pass
        
        # Azure / OpenAI-like: resp.choices[0].message.content or resp.choices[0].text
        if hasattr(resp, "choices"):
            c = resp.choices
            if isinstance(c, (list, tuple)) and len(c):
                first = c[0]
                if hasattr(first, "message"):
                    if hasattr(first.message, "get"):
                        content = first.message.get("content", "")
                    elif hasattr(first.message, "content"):
                        content = first.message.content
                    else:
                        content = str(first.message)
                    logger.debug("[SCOPE] LLM ì‘ë‹µ ì¶”ì¶œ: choices[0].message")
                    return content
                if hasattr(first, "text"):
                    content = getattr(first, "text", "")
                    logger.debug("[SCOPE] LLM ì‘ë‹µ ì¶”ì¶œ: choices[0].text")
                    return content
        
        # some SDKs use .content directly
        if hasattr(resp, "content"):
            content = getattr(resp, "content")
            logger.debug("[SCOPE] LLM ì‘ë‹µ ì¶”ì¶œ: .content ì†ì„±")
            return content if isinstance(content, str) else str(content)
        
        # fallback to string conversion
        result = str(resp)
        logger.debug("[SCOPE] LLM ì‘ë‹µ ì¶”ì¶œ: str() ë³€í™˜")
        return result
    except Exception as e:
        logger.warning("[SCOPE] raw extract failed: %s", e)
        return str(resp) if resp else ""

# ---------------------------------------------------------------------
# JSON íŒŒì„œ #1107
# ---------------------------------------------------------------------
def _json_from_text(maybe: str) -> Optional[dict]:
    """ë¬¸ìì—´ì—ì„œ JSON ì¶”ì¶œ (Markdown ì½”ë“œ ë¸”ë¡ ì§€ì›)"""
    if not maybe:
        return None
    
    try:
        s = maybe.strip()
        
        # â­ Markdown ì½”ë“œ ë¸”ë¡ ì œê±°
        # ```json\n{...}\n``` â†’ {...}
        s = re.sub(r'```json\s*', '', s)
        s = re.sub(r'```\s*', '', s)
        s = s.strip()
        
        # JSON íŒŒì‹±
        if s.startswith("{") and s.endswith("}"):
            result = json.loads(s)
            req_count = len(result.get("requirements", []))
            logger.info(f"âœ… [SCOPE] JSON íŒŒì‹± ì„±ê³µ (requirements={req_count})")
            return result
        
        # ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œ
        m = re.search(r"(\{[\s\S]*\})", s)
        if m:
            result = json.loads(m.group(1))
            req_count = len(result.get("requirements", []))
            logger.info(f"âœ… [SCOPE] ì •ê·œì‹ ì¶”ì¶œ ì„±ê³µ (requirements={req_count})")
            return result
            
    except json.JSONDecodeError as e:
        logger.error(f"[SCOPE] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        logger.error(f"[SCOPE] ì‘ë‹µ ì²˜ìŒ 500ì:\n{maybe[:500]}")
    except Exception as e:
        logger.error(f"[SCOPE] ì˜ˆì™¸: {e}")
    
    return None

# ============================================================================
# 1. _estimate_confidence í•¨ìˆ˜ ìˆ˜ì • (Line 172-202) #1107 confidence ë¬´ì‹œí•˜ê³  íŒŒì‹±
# ============================================================================

def _estimate_confidence(resp_json: Optional[dict], raw_text: str) -> float:
    """
    ê°œì„ ëœ confidence ì¶”ì •ê¸°:
    - ìš”êµ¬ì‚¬í•­ì´ ì—†ìœ¼ë©´ ë§¤ìš° ë‚®ì€ ì ìˆ˜ (0.1)
    - ìš”êµ¬ì‚¬í•­ ìˆ˜ì™€ í•„ë“œ ì™„ì „ì„±ì„ ëª¨ë‘ ê³ ë ¤
    - acceptance_criteria ì¡´ì¬ ì—¬ë¶€ë„ ì²´í¬
    """
    if resp_json and isinstance(resp_json, dict):
        # direct provided confidence
        if "confidence" in resp_json:
            try:
                c = float(resp_json["confidence"])
                return min(max(c, 0.0), 1.0)
            except Exception:
                pass
        
        # heuristic: requirements ìˆ˜ì™€ í’ˆì§ˆ
        reqs = resp_json.get("requirements")
        if not reqs or not isinstance(reqs, list):
            logger.debug("[SCOPE] confidence: requirementsê°€ ì—†ê±°ë‚˜ listê°€ ì•„ë‹˜")
            return 0.1  # ìš”êµ¬ì‚¬í•­ì´ ì—†ìœ¼ë©´ ë§¤ìš° ë‚®ì€ ì ìˆ˜
        
        if len(reqs) == 0:
            logger.debug("[SCOPE] confidence: requirements ë°°ì—´ì´ ë¹„ì–´ìˆìŒ")
            return 0.1  # ë¹ˆ ë°°ì—´ë„ ë§¤ìš° ë‚®ì€ ì ìˆ˜
        
        # ìš”êµ¬ì‚¬í•­ ìˆ˜ì— ë”°ë¥¸ ê¸°ë³¸ ì ìˆ˜
        if len(reqs) < 3:
            base_score = 0.3  # ë„ˆë¬´ ì ìŒ
        elif len(reqs) < 5:
            base_score = 0.5  # ì ìŒ
        elif len(reqs) < 10:
            base_score = 0.6  # ë³´í†µ
        else:
            base_score = 0.7  # ì¶©ë¶„
        
        # í•„ë“œ ì™„ì „ì„± ì²´í¬
        filled = 0
        has_ac = 0  # acceptance_criteria ìˆëŠ” ê²ƒ
        
        for r in reqs:
            # í•„ìˆ˜ í•„ë“œ
            has_required = (
                r.get("req_id") and 
                r.get("title") and 
                r.get("description") and
                r.get("type") and
                r.get("priority")
            )
            
            if has_required:
                filled += 1
            
            # acceptance_criteria ì²´í¬
            ac = r.get("acceptance_criteria")
            if ac and isinstance(ac, list) and len(ac) >= 2:
                has_ac += 1
        
        if len(reqs) == 0:
            return 0.1
        
        field_ratio = filled / len(reqs)  # í•„ìˆ˜ í•„ë“œ ì¶©ì¡±ë¥ 
        ac_ratio = has_ac / len(reqs)     # acceptance_criteria ì¶©ì¡±ë¥ 
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        # base_score (0.3-0.7) + field_ratio (0-0.2) + ac_ratio (0-0.1)
        final_score = base_score + (field_ratio * 0.2) + (ac_ratio * 0.1)
        
        logger.debug(
            f"[SCOPE] confidence: {len(reqs)}ê°œ ìš”êµ¬ì‚¬í•­, "
            f"í•„ë“œ ì¶©ì¡± {filled}/{len(reqs)}, "
            f"AC ì¶©ì¡± {has_ac}/{len(reqs)}, "
            f"ì ìˆ˜ {final_score:.3f}"
        )
        
        return min(final_score, 0.99)
    
    # fallback: JSON íŒŒì‹± ì‹¤íŒ¨
    logger.debug("[SCOPE] confidence: JSON íŒŒì‹± ì‹¤íŒ¨")
    return 0.1

def _ensure_req_ids(reqs: List[dict]) -> List[dict]:
    """req_idê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±"""
    out = []
    ts = int(time.time())
    counter = 0
    for r in reqs:
        if not r.get("req_id"):
            counter += 1
            r["req_id"] = f"REQ-{ts}-{counter:02}"
        out.append(r)
    return out

# ---------------------------------------------------------------------
# 1111 PromptManager: RAG + ì••ì¶• + ìºì‹±
# ---------------------------------------------------------------------
class PromptManager:
    def __init__(self):
        self.llm = get_llm()
        self.vectorstore = None
        self._init_vectorstore()

    def _init_vectorstore(self):
        """ templates/ ë° rules í´ë”ë¥¼ ë²¡í„°í™”. í‚¤ ì—†ìœ¼ë©´ RAG ë¹„í™œì„± """
        texts, metas = [], []
        for p in Path("templates").rglob("*.txt"):
            t = p.read_text(encoding="utf-8", errors="ignore")
            texts.append(t); metas.append({"name": str(p.relative_to("templates"))})
        for p in Path("rules").rglob("*.txt"):
            t = p.read_text(encoding="utf-8", errors="ignore")
            texts.append(t); metas.append({"name": str(p.relative_to("rules"))})
        if not texts: 
            logger.info("[PROMPT-RAG] í…ìŠ¤íŠ¸ê°€ ì—†ì–´ RAG ìƒëµ")
            self.vectorstore = None    
            return
        
        # 1111 ğŸ”‘ í™˜ê²½ ë³€ìˆ˜ ê°ì§€ (Azure ìš°ì„ )
        azure_key = os.getenv("AZURE_OPENAI_API_KEY")
        azure_ep  = os.getenv("AZURE_OPENAI_ENDPOINT")
        azure_ver = os.getenv("OPENAI_API_VERSION") or os.getenv("AZURE_OPENAI_API_VERSION")
        azure_embed_deploy = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT")  # ì˜ˆ: text-embedding-3-large

        openai_key = os.getenv("OPENAI_API_KEY")
        try:
            if azure_key and azure_ep and azure_ver and azure_embed_deploy:
                emb = AzureOpenAIEmbeddings(
                    azure_endpoint=azure_ep,
                    api_key=azure_key,
                    api_version=azure_ver,
                    deployment=azure_embed_deploy,
                )
                logger.info("[PROMPT-RAG] AzureOpenAIEmbeddings ì‚¬ìš©")
            elif openai_key:
                emb = OpenAIEmbeddings(  # langchain_openai
                    model=os.getenv("OPENAI_EMBED_MODEL", "text-embedding-3-large"),
                    api_key=openai_key,
                )
                logger.info("[PROMPT-RAG] OpenAIEmbeddings ì‚¬ìš©")
            else:
                logger.warning("[PROMPT-RAG] ì„ë² ë”© í‚¤ ì—†ìŒ â†’ RAG ë¹„í™œì„±")
                self.vectorstore = None
                return

            self.vectorstore = FAISS.from_texts(texts, metadatas=metas, embedding=emb)
            logger.info(f"[PROMPT-RAG] ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ ({len(texts)} docs)")
        except Exception as e:
            logger.warning(f"[PROMPT-RAG] ì´ˆê¸°í™” ì‹¤íŒ¨ â†’ RAG ë¹„í™œì„±: {e}")
            self.vectorstore = None

    def build_rag_prompt(self, text: str, base_prompt=None, k=3) -> str:
        """ì‚¬ìš©ì ë¬¸ì„œì™€ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í…œí”Œë¦¿/ë£°ì„ ì°¾ì•„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        if not self.vectorstore:
            return base_prompt or build_scope_prompt(text)
        results = self.vectorstore.similarity_search(text, k=k)
        retrieved = "\n\n".join(r.page_content for r in results)
        base = base_prompt or "ë‹¹ì‹ ì€ PMP í‘œì¤€ì„ ì¤€ìˆ˜í•˜ëŠ” PM ë¶„ì„ê°€ì…ë‹ˆë‹¤."
        if not self.vectorstore:
            # RAG ë¹„í™œì„± ì‹œì—ë„ ì •ìƒ ë™ì‘
            return f"{base}\n\në¬¸ì„œ:\n{text[:8000]}"
        results = self.vectorstore.similarity_search(text, k=k)
        retrieved = "\n\n".join(r.page_content for r in results)
        return f"{base}\n\n{retrieved}\n\në¬¸ì„œ:\n{text[:8000]}"

    def compress_prompt(self, prompt: str) -> str:
        if not self.llm or len(prompt) < 10000:
            return prompt
        try:
            msg = [{"role": "user", "content": f"ì•„ë˜ í”„ë¡¬í”„íŠ¸ë¥¼ 50% ê¸¸ì´ë¡œ ì••ì¶•:\n{prompt}"}]
            resp = self.llm.invoke(msg)
            return _safe_extract_raw(resp)
        except Exception as e:
            logger.warning(f"[PROMPT-RAG] ì••ì¶• ì‹¤íŒ¨: {e}")
            return prompt
        

# ---------------------------------------------------------------------
# ScopeAgent (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€ + ê°œì„  í†µí•©)
# ---------------------------------------------------------------------
class ScopeAgent:
    """RFP ë¬¸ì„œë¡œë¶€í„° Requirements/SRS/RTM/WBS(ì´ˆì•ˆ) ë“±ì„ ìƒì„±í•˜ëŠ” Agent
       ì¶”ê°€ ì˜µì…˜ (payload['options']):
         - confidence_threshold: float (0..1), default=0.75
         - max_attempts: int, default=3
    """

    def __init__(self, data_dir: Optional[str] = None):
        self.llm = get_llm()
        self.pmgr = PromptManager()  # 1111
        self.data_dir = data_dir or "data"
        logger.info(f"[SCOPE_AGENT] ì´ˆê¸°í™” ì™„ë£Œ1 - data_dir: {self.data_dir}")

    async def _call_llm(self, prompt: str):
        if not self.llm:
            raise RuntimeError("LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if hasattr(self.llm, "invoke"):
            msgs = [
                {"role": "system", "content": "You are a PM analyst."},
                {"role": "user", "content": prompt, "cache_control": {"type": "ephemeral"}}
            ]
            return await asyncio.to_thread(self.llm.invoke, msgs)
        else:
            return await asyncio.to_thread(self.llm, prompt)

    async def _extract_items_with_confidence(self, text: str, threshold=0.75, max_attempts=3):
        attempt, last_json, last_raw = 0, None, ""
        while attempt < max_attempts:
            attempt += 1
            logger.info(f"[SCOPE] ì‹œë„ {attempt}/{max_attempts}")
            prompt = self.pmgr.build_rag_prompt(text) if not last_json else \
                f"ì´ì „ ê²°ê³¼ ê°œì„ :\n{json.dumps(last_json, ensure_ascii=False)[:1500]}\n\n{text[:6000]}"
            prompt = self.pmgr.compress_prompt(prompt)
            try:
                resp = await self._call_llm(prompt)
                raw = _safe_extract_raw(resp)
                parsed = _json_from_text(raw)
                conf = _estimate_confidence(parsed, raw)
                if parsed and conf >= threshold:
                    logger.info(f"âœ… ì„±ê³µ: conf={conf:.2f}")
                    return parsed, raw
                last_json, last_raw = parsed, raw
                await asyncio.sleep(0.3)
            except Exception as e:
                logger.error(f"[SCOPE] LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                await asyncio.sleep(0.5)
        logger.warning("[SCOPE] ìµœëŒ€ ì‹œë„ ë„ë‹¬. ë§ˆì§€ë§‰ ê²°ê³¼ ë°˜í™˜.")
        return last_json or {"requirements": []}, last_raw

    async def pipeline(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        project_id = payload.get("project_id") or payload.get("project_name") or "Unknown"
        text = payload.get("text") or ""
        documents = payload.get("documents") or []
        options = payload.get("options") or {}
        confidence_threshold = float(options.get("confidence_threshold", 0.75))
        max_attempts = int(options.get("max_attempts", 3))

        # if no text but documents - read first file path if exists (best-effort)
        if not text and documents:
            first = documents[0]
            path = None
            if isinstance(first, dict):
                path = first.get("path")
            else:
                path = getattr(first, "path", None)
            if path:
                p = Path(path)
                if not p.exists():
                    # maybe relative to data/inputs/RFP
                    alt = Path("data/inputs/RFP") / Path(path).name
                    if alt.exists():
                        p = alt
                if p.exists():
                    try:
                        # simple read for txt; for docx/pdf we expect upper layer to convert
                        text = p.read_text(encoding="utf-8", errors="ignore")
                    except Exception:
                        text = ""

        logger.info("ğŸ”µ [SCOPE] ìš”ì²­: project_id=%s, methodology=%s", project_id, payload.get("methodology"))

        # Run extraction with confidence loop
        items, raw_resp = await self._extract_items_with_confidence(text, confidence_threshold, max_attempts)

        # Ensure req_ids
        reqs = items.get("requirements", [])
        if reqs:
            items["requirements"] = _ensure_req_ids(reqs)

        # Write outputs: srs, scope md, rtm csv, wbs json draft
        out_dir = Path("data/outputs/scope") / str(project_id)
        out_dir.mkdir(parents=True, exist_ok=True)

        srs_path = out_dir / f"{project_id}_SRS.md"
        self._generate_srs(project_id, items, srs_path)

        # WBS draft: keep simple hierarchical draft (could be improved via WBS synthesis step)
        wbs = await self._synthesize_wbs_draft(items, depth=int(options.get("wbs_depth", 3)))
        wbs_path = out_dir / "wbs_structure.json"
        wbs_path.write_text(json.dumps(wbs, ensure_ascii=False, indent=2), encoding="utf-8")

        # RTM csv
        rtm_csv = out_dir / "rtm.csv"
        with rtm_csv.open("w", encoding="utf-8", newline="") as fh:
            fh.write("req_id,wbs_id,test_case,verification_status\n")
            for r in items.get("requirements", []):
                fh.write(f"{r.get('req_id')},,,Candidate\n")

        # attempt DB save (best-effort)
        saved = 0
        if _DB_AVAILABLE:
            try:
                saved = self._save_requirements_db(project_id, items)
            except Exception as e:
                logger.exception("[SCOPE] DB save failed: %s", e)
                saved = 0
        else:
            logger.debug("[SCOPE] DB not available, skipping DB save")

        # PMP outputs (scope_statement excel etc.) - keep existing hooks
        pmp_outputs = await self._generate_pmp_outputs(project_id, items, wbs, options, out_dir)

        result = {
            "status": "ok",
            "project_id": project_id,
            "requirements": items.get("requirements", []),
            "functions": items.get("functions", []),
            "wbs_json": str(wbs_path),
            "wbs": wbs,
            "rtm_csv": str(rtm_csv),
            "srs_path": str(srs_path),
            "pmp_outputs": pmp_outputs,
            "db_saved_requirements": saved,
            "_llm_raw_response": str(raw_resp)[:2000],
        }
        logger.info("âœ… [SCOPE] ì‘ë‹µì™„ë£Œ: %s (requirements=%d, saved=%d)", project_id, len(items.get("requirements", [])), saved)
        return result


    def _fallback_extract(self, text: str) -> Dict[str, Any]:
        """ê°„ë‹¨ ê·œì¹™ ê¸°ë°˜ (ê¸°ì¡´ fallback ìœ ì§€)"""
        reqs = []
        funcs = []
        for i, ln in enumerate([l.strip() for l in text.splitlines() if l.strip()]):
            if re.search(r"(ìš”êµ¬|í•„ìš”|í•´ì•¼|shall|must|should)", ln, re.I):
                reqs.append({
                    "req_id": None,
                    "title": ln[:80],
                    "type": "functional",
                    "priority": "Medium",
                    "description": ln,
                    "source_span": f"line {i+1}"
                })
            if re.search(r"(ê¸°ëŠ¥|ê¸°ëŠ¥ëª…|support|provide)", ln, re.I):
                funcs.append({"name": ln[:80], "desc": ln})
        logger.info("âœ… [SCOPE] fallback ì¶”ì¶œ: %d reqs, %d funcs", len(reqs), len(funcs))
        return {"requirements": reqs, "functions": funcs}

    async def _synthesize_wbs_draft(self, items: Dict[str, Any], depth: int = 3) -> Dict[str, Any]:
        """ê°„ë‹¨í•œ WBS ì´ˆì•ˆ ìƒì„± (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        nodes = [{
            "id": "WBS-1",
            "name": "Project",
            "level": 1,
            "children": []
        }]
        phases = []
        reqs = items.get("requirements", [])
        # naive: split into phases of ~ceil(len(reqs)/3)
        if reqs:
            per = max(1, (len(reqs) + 2) // 3)
            for i in range(3):
                start = i * per
                seg = reqs[start:start+per]
                phase = {
                    "id": f"WBS-1.{i+1}",
                    "name": f"Phase {i+1}",
                    "level": 2,
                    "children": []
                }
                # tasks per requirement
                for j, r in enumerate(seg, 1):
                    phase["children"].append({
                        "id": f"{phase['id']}.{j}",
                        "name": r.get("title", f"Task {i+1}.{j}")[:60],
                        "level": 3,
                        "owner": None,
                        "deliverables": None
                    })
                phases.append(phase)
        else:
            # default phases
            for i in range(3):
                phases.append({
                    "id": f"WBS-1.{i+1}",
                    "name": f"Phase {i+1}",
                    "level": 2,
                    "children": []
                })
        nodes[0]["children"] = phases
        return {"nodes": nodes, "depth": depth}

    def _save_requirements_db(self, project_id: str, items: Dict[str, Any]) -> int:
        """
        DBì— ìš”êµ¬ì‚¬í•­ ì €ì¥(ê°„ë‹¨ êµ¬í˜„). ë°˜í™˜: ì €ì¥ëœ ë ˆì½”ë“œ ìˆ˜.
        ì•ˆì „ ì¥ì¹˜: req_idê°€ ë¹„ì–´ìˆìœ¼ë©´ ìë™ìƒì„± í›„ ì €ì¥.
        """
        if not _DB_AVAILABLE:
            logger.debug("[SCOPE] DB not available")
            return 0
        db = SessionLocal()
        saved = 0
        try:
            reqs = items.get("requirements", []) or []
            for r in reqs:
                req_id = r.get("req_id")
                title = r.get("title") or r.get("description")[:200]
                description = r.get("description")
                rtype = r.get("type") or r.get("category") or "functional"
                priority = r.get("priority") or "Medium"
                source_doc = r.get("source_span") or None

                if not req_id:
                    req_id = f"AUTO-{int(time.time())}-{saved+1}"
                    logger.debug("[SCOPE] ìë™ req_id ìƒì„±: %s", req_id)

                # upsert-like: try find existing by project_id + req_id
                existing = db.query(pm_models.PM_Requirement).filter_by(project_id=project_id, req_id=req_id).first()
                if existing:
                    existing.title = title
                    existing.description = description
                    existing.priority = priority
                    existing.type = rtype
                    existing.source_doc = source_doc
                else:
                    obj = pm_models.PM_Requirement(
                        project_id=project_id,
                        req_id=req_id,
                        title=title,
                        description=description,
                        priority=priority,
                        status="new",
                        source_doc=source_doc,
                        created_at=datetime.utcnow()
                    )
                    db.add(obj)
                saved += 1
            db.commit()
            logger.info("[SCOPE] DB ì €ì¥ ì™„ë£Œ: %d ë ˆì½”ë“œ", saved)
            return saved
        except Exception as e:
            logger.exception("[SCOPE] Saving to DB failed: %s", e)
            try:
                db.rollback()
            except Exception:
                pass
            return 0
        finally:
            db.close()

    # SRS / Charter / PMP outputs (existing hooks)
    def _generate_srs(self, project_id: Any, items: Dict[str, Any], out_path: Path):
        out_path.parent.mkdir(parents=True, exist_ok=True)
        with open(out_path, "w", encoding="utf-8") as f:
            f.write(f"# Software Requirements Specification\n")
            f.write(f"**Project:** {project_id}\n")
            f.write(f"**Generated:** {datetime.utcnow().isoformat()}\n\n")
            f.write("## 1. Requirements\n\n")
            for r in items.get("requirements", []):
                f.write(f"### {r.get('req_id')}: {r.get('title')}\n")
                f.write(f"- **Type:** {r.get('type')}\n")
                f.write(f"- **Priority:** {r.get('priority')}\n")
                f.write(f"- **Description:** {r.get('description')}\n")
                f.write(f"- **Source:** {r.get('source_span')}\n\n")
        return str(out_path)

    # def _generate_pmp_outputs(self, project_id: Any, project_dir: Path, requirements: Dict[str, Any]) -> Dict[str, Optional[str]]:
    #     outputs = {}
    #     try:
    #         from .outputs.scope_statement import ScopeStatementGenerator
    #         scp = project_dir / f"{project_id}_ScopeStatement.xlsx"
    #         outputs["scope_statement_excel"] = ScopeStatementGenerator.generate(project_id, requirements, scp)
    #     except Exception as e:
    #         outputs["scope_statement_excel"] = None
    #         logger.debug("ScopeStatementGenerator not available: %s", e)
    #     return outputs
    async def _generate_pmp_outputs(self, project_id: str, items: dict, wbs_data: dict, options: dict, out_dir: Path):
        """Scope ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ë¬¸ì„œ ì‚°ì¶œë¬¼ ìƒì„±"""
        try:
            reqs = items.get("requirements", [])
            logger.info(f"[SCOPE] ğŸ“¦ ì‚°ì¶œë¬¼ ìƒì„± ì‹œì‘ - {len(reqs)}ê°œ ìš”êµ¬ì‚¬í•­")
            req_summary = "\n".join([
                f"- {r['req_id']}: {r['title']} ({r['type']}, {r['priority']})"
                for r in reqs[:10]
            ])
            charter_base = PROJECT_CHARTER_PROMPT.format(
                project_name=project_id,
                sponsor=options.get("sponsor", "ë¯¸ì •"),
                background=options.get("background", "íšŒì‚¬ ë‚´ë¶€ ìš”ì²­"),
                objectives=options.get("objectives", "ëª…í™•í•œ ìš”êµ¬ì‚¬í•­ ë„ì¶œ ë° ì‹œìŠ¤í…œ í’ˆì§ˆ í™•ë³´"),
                requirements_summary=req_summary
            )
            # 1ï¸âƒ£ í”„ë¡œì íŠ¸ í—Œì¥ (Word)
            charter_path = out_dir / f"{project_id}_í”„ë¡œì íŠ¸í—Œì¥.docx"
            ProjectCharterGenerator.generate(
                project_name=project_id,
                requirements=reqs,
                wbs_data=wbs_data,
            )
            logger.info(f"[SCOPE] âœ… í”„ë¡œì íŠ¸ í—Œì¥ ìƒì„±: {charter_path}")

            # 2ï¸âƒ£ ë²”ìœ„ ê¸°ìˆ ì„œ (Excel)
            scope_stmt_path = out_dir / f"{project_id}_ë²”ìœ„ê¸°ìˆ ì„œ.xlsx"
            ScopeStatementGenerator.generate(
                project_name=project_id,
                wbs_data=wbs_data,
                requirements=reqs,
                output_path=scope_stmt_path
            )
            logger.info(f"[SCOPE] âœ… ë²”ìœ„ ê¸°ìˆ ì„œ ìƒì„±: {scope_stmt_path}")

            # 3ï¸âƒ£ ìš”êµ¬ì‚¬í•­ ì¶”ì í‘œ (RTM)
            rtm_path = out_dir / f"{project_id}_ìš”êµ¬ì‚¬í•­ì¶”ì í‘œ.xlsx"
            RTMExcelGenerator.generate(
                requirements=reqs,
                output_path=rtm_path
            )
            logger.info(f"[SCOPE] âœ… RTM ìƒì„±: {rtm_path}")

            # 4ï¸âƒ£ WBS Excel
            wbs_excel_path = out_dir / f"{project_id}_WBS.xlsx"
            WBSExcelGenerator.generate(
                wbs_data=wbs_data,
                output_path=wbs_excel_path
            )
            logger.info(f"[SCOPE] âœ… WBS Excel ìƒì„±: {wbs_excel_path}")

            # 5ï¸âƒ£ Tailoring (ë°©ë²•ë¡ ë³„)
            tailoring_path = out_dir / f"{project_id}_í…Œì¼ëŸ¬ë§.xlsx"
            TailoringGenerator.generate(
                methodology=options.get("methodology", "waterfall"),
                requirements=reqs, 
                output_path=tailoring_path
            )
            logger.info(f"[SCOPE] âœ… Tailoring ìƒì„±: {tailoring_path}")

            # 6ï¸âƒ£ ì‚¬ì—…ìˆ˜í–‰ê³„íšì„œ (Project Plan)
            plan_path = out_dir / f"{project_id}_ì‚¬ì—…ìˆ˜í–‰ê³„íšì„œ.xlsx"
            ProjectPlanGenerator.generate(
                project_name=project_id,
                requirements=reqs,
                wbs_data=wbs_data,
                options=options,
                output_path=plan_path
            )
            logger.info(f"[SCOPE] âœ… ì‚¬ì—…ìˆ˜í–‰ê³„íšì„œ ìƒì„±: {plan_path}")

        except Exception as e:
            logger.error(f"[SCOPE] âŒ ì‚°ì¶œë¬¼ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        

    async def _synthesize_wbs_draft(self, items, depth=3):
        reqs = items.get("requirements", [])
        nodes = [{"id": "WBS-1", "name": "Project", "level": 1, "children": []}]
        per = max(1, (len(reqs) + 2) // 3)
        for i in range(3):
            phase = {"id": f"WBS-1.{i+1}", "name": f"Phase {i+1}", "level": 2, "children": []}
            for j, r in enumerate(reqs[i*per:(i+1)*per], 1):
                phase["children"].append({
                    "id": f"{phase['id']}.{j}",
                    "name": r.get("title", f"Task {i+1}.{j}")[:60],
                    "level": 3
                })
            nodes[0]["children"].append(phase)
        return {"nodes": nodes, "depth": depth}

    async def _generate_project_documents(self, project_id: str, options: dict, out_dir: Path):
        """
        ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ Project Charter, Tailoring Guide, WBS ë¬¸ì„œë¥¼ ìƒì„±
        """
        logger.info(f"[SCOPE] ğŸ“„ Project Charter / Tailoring / WBS ìƒì„± ì‹œì‘")

        # ìš”êµ¬ì‚¬í•­ íŒŒì¼ ë¡œë“œ
        req_path = out_dir / "requirements.json"
        req_json = {}
        if req_path.exists():
            try:
                req_json = json.loads(req_path.read_text(encoding="utf-8"))
            except Exception as e:
                logger.warning(f"[SCOPE] ìš”êµ¬ì‚¬í•­ JSON ë¡œë“œ ì‹¤íŒ¨: {e}")

        # 1ï¸âƒ£ Charter ìƒì„±
        try:
            charter_base = PROJECT_CHARTER_PROMPT.format(
                project_name=project_id,
                sponsor=options.get("sponsor", "ë¯¸ì •"),
                background=options.get("background", "íšŒì‚¬ ë‚´ë¶€ ìš”ì²­"),
                objectives=options.get("objectives", "ëª…í™•í•œ ìš”êµ¬ì‚¬í•­ ë„ì¶œ ë° ì‹œìŠ¤í…œ í’ˆì§ˆ í™•ë³´")
            )
            charter_prompt = self.pmgr.build_rag_prompt(charter_base)
            charter_prompt = self.pmgr.compress_prompt(charter_prompt)

            resp = await asyncio.to_thread(
                self.llm.invoke,
                [
                    {"role": "system", "content": "You are a PMO documentation expert."},
                    {"role": "user", "content": charter_prompt}
                ]
            )
            charter_text = _safe_extract_raw(resp)
            (out_dir / "project_charter.md").write_text(charter_text, encoding="utf-8")
            logger.info(f"[SCOPE] âœ… Project Charter ìƒì„± ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"[SCOPE] âš ï¸ Charter ìƒì„± ì‹¤íŒ¨: {e}")

        # 2ï¸âƒ£ Tailoring Guide ìƒì„±
        try:
        # 1111
            func_count = len([r for r in reqs if r["type"] == "functional"])
            nonfunc_count = len([r for r in reqs if r["type"] == "non-functional"])
            constraint_count = len([r for r in reqs if r["type"] == "constraint"])
            tailoring_base = TAILORING_PROMPT.format(
                req_count=len(reqs),
                func_count=func_count,
                nonfunc_count=nonfunc_count,
                constraint_count=constraint_count,
                size=options.get("size", "ì¤‘í˜•"),
                methodology=options.get("methodology", "Waterfall"),
                complexity=options.get("complexity", "ì¤‘ê°„"),
                team_size=options.get("team_size", "8"),
                duration=options.get("duration", "6")
)
            tailoring_prompt = self.pmgr.build_rag_prompt(tailoring_base)
            tailoring_prompt = self.pmgr.compress_prompt(tailoring_prompt)

            resp = await asyncio.to_thread(
                self.llm.invoke,
                [
                    {"role": "system", "content": "You are a PMP process tailoring expert."},
                    {"role": "user", "content": tailoring_prompt}
                ]
            )
            tailoring_text = _safe_extract_raw(resp)
            (out_dir / "tailoring_guide.json").write_text(tailoring_text, encoding="utf-8")
            logger.info(f"[SCOPE] âœ… Tailoring Guide ìƒì„± ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"[SCOPE] âš ï¸ Tailoring ìƒì„± ì‹¤íŒ¨: {e}")

        # 3ï¸âƒ£ ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ WBS ìƒì„±
        try:
            from server.workflow.agents.schedule_agent.prompts import WBS_SYNTH_PROMPT

            if req_json.get("requirements"):
                req_text = json.dumps(req_json["requirements"][:20], ensure_ascii=False, indent=2)
                wbs_prompt = WBS_SYNTH_PROMPT.format(requirements_json=req_text)
                wbs_prompt = self.pmgr.build_rag_prompt(wbs_prompt)
                wbs_prompt = self.pmgr.compress_prompt(wbs_prompt)

                resp = await asyncio.to_thread(
                    self.llm.invoke,
                    [
                        {"role": "system", "content": "You are a project scheduling expert."},
                        {"role": "user", "content": wbs_prompt}
                    ]
                )
                wbs_raw = _safe_extract_raw(resp)
                match = re.search(r"(\{[\s\S]*\})", wbs_raw)
                if match:
                    wbs_json = json.loads(match.group(1))
                    (out_dir / "wbs_structure.json").write_text(
                        json.dumps(wbs_json, ensure_ascii=False, indent=2), encoding="utf-8"
                    )
                    logger.info(f"[SCOPE] âœ… WBS êµ¬ì¡° ìƒì„± ì™„ë£Œ: {len(wbs_json.get('nodes', []))}ê°œ ë…¸ë“œ")
        except Exception as e:
            logger.warning(f"[SCOPE] âš ï¸ WBS ìƒì„± ì‹¤íŒ¨: {e}")

        logger.info(f"[SCOPE] ğŸ“¦ Project ë¬¸ì„œ ìƒì„± ì™„ë£Œ: {project_id}")


# ---------------------------------------------------------------------
# ì²´ì¸ íŒŒì´í”„ë¼ì¸ (Scope â†’ Quality â†’ Schedule)
# ---------------------------------------------------------------------
class ScopeChainPipeline:
    def __init__(self, scope_agent, quality_agent, schedule_agent):
        self.scope_agent = scope_agent
        self.quality_agent = quality_agent
        self.schedule_agent = schedule_agent

    async def run(self, text, project_meta=None):
        logger.info("ğŸš€ ì²´ì¸ íŒŒì´í”„ë¼ì¸ ì‹œì‘")

        # 1ï¸âƒ£ Scope ì¶”ì¶œ (RAG + Few-shot)
        scope_res = await self.scope_agent._extract_items_with_confidence(text)
        reqs = scope_res[0].get("requirements", [])
        logger.info(f"ğŸ“„ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ ì™„ë£Œ: {len(requirements)}ê°œ")
        
        # 2ï¸âƒ£ í’ˆì§ˆ ê²€ì¦
        valid = self.quality_agent.validate(reqs, text, project_meta)
        logger.info(f"âœ… í’ˆì§ˆ ì ìˆ˜: {validation['score']} ({validation['grade']})")

        if not valid.get("pass", True):
            # ìë™ ê°œì„  ë£¨í”„
            logger.info("ğŸ”„ í’ˆì§ˆ ë¯¸ë‹¬ â†’ ì¬ì¶”ì¶œ ì‹œë„")
            reqs = self.scope_agent.refine_requirements(text, reqs, valid, project_meta)
        
        # 3ï¸âƒ£ ìŠ¤ì¼€ì¤„ ì´ˆì•ˆ (ScheduleAgent ì—°ë™)
        wbs = await self.schedule_agent.generate_wbs(reqs)
        logger.info(f"ğŸ“… WBS ìƒì„± ì™„ë£Œ: {len(wbs_draft.get('nodes', []))}ë‹¨ê³„")
        return {"requirements": reqs, "validation": valid, "wbs": wbs, "status": "complete"}

        # 4 Output ìƒì„± 
        wbs = await self.schedule_agent.generate_wbs(reqs)
        logger.info(f"ğŸ“… WBS ìƒì„± ì™„ë£Œ: {len(wbs_draft.get('nodes', []))}ë‹¨ê³„")
        return {"requirements": reqs, "validation": valid, "wbs": wbs, "status": "complete"}


    # ScopeAgentì— ì¶”ê°€í•  ë©”ì„œë“œë“¤
    # pipeline.pyì˜ ScopeAgent í´ë˜ìŠ¤ì— ì¶”ê°€

    def refine_requirements(self, 
                        text: str, 
                        previous_result: List[Dict[str, Any]],
                        validation_result: Dict[str, Any],
                        project_meta: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        ê²€ì¦ í”¼ë“œë°±ì„ ë°˜ì˜í•œ ìš”êµ¬ì‚¬í•­ ì¬ì¶”ì¶œ
        
        Args:
            text: ì›ë³¸ ë¬¸ì„œ
            previous_result: ì´ì „ ì¶”ì¶œ ê²°ê³¼
            validation_result: í’ˆì§ˆ ê²€ì¦ ê²°ê³¼
            project_meta: í”„ë¡œì íŠ¸ ë©”íƒ€ë°ì´í„°
        
        Returns:
            ê°œì„ ëœ ìš”êµ¬ì‚¬í•­ ë¦¬ìŠ¤íŠ¸
        """
        logger.info("[SCOPE] í”¼ë“œë°± ê¸°ë°˜ ì¬ì¶”ì¶œ ì‹œì‘")
        
        # ê²€ì¦ ê²°ê³¼ ë¶„ì„
        score = validation_result.get('score', 0)
        issues = validation_result.get('issues', [])
        missing = validation_result.get('missing_requirements', [])
        recommendations = validation_result.get('recommendations', [])
        
        logger.info(f"[SCOPE] ì´ì „ ì ìˆ˜: {score}")
        logger.info(f"[SCOPE] ì´ìŠˆ: {len(issues)}ê°œ")
        logger.info(f"[SCOPE] ëˆ„ë½: {len(missing)}ê°œ")
        
        # í”¼ë“œë°± í”„ë¡¬í”„íŠ¸ ìƒì„±
        feedback_section = self._build_feedback_section(
            score, issues, missing, recommendations
        )
        
        # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
        refinement_prompt = f"""
    ë‹¹ì‹ ì€ PMP í‘œì¤€ì„ ì¤€ìˆ˜í•˜ëŠ” ì „ë¬¸ PMO ë¶„ì„ê°€ì…ë‹ˆë‹¤.

    ## ğŸ”„ ìš”êµ¬ì‚¬í•­ ê°œì„  ì‘ì—…

    ### ì´ì „ ì¶”ì¶œ ê²°ê³¼
    ì¶”ì¶œëœ ìš”êµ¬ì‚¬í•­: {len(previous_result)}ê°œ
    í’ˆì§ˆ ì ìˆ˜: {score}/100

    ### ğŸ“‹ ì´ì „ ì¶”ì¶œ ê²°ê³¼ (ìš”ì•½)
    {self._summarize_requirements(previous_result)}

    ### âš ï¸ ê²€ì¦ì—ì„œ ë°œê²¬ëœ ë¬¸ì œì 

    {feedback_section}

    ### ğŸ¯ ê°œì„  ì§€ì¹¨

    1. **ëˆ„ë½ëœ ìš”êµ¬ì‚¬í•­ ì¶”ê°€**
    {self._format_missing(missing)}

    2. **ê¸°ì¡´ ìš”êµ¬ì‚¬í•­ ê°œì„ **
    {self._format_issues(issues)}

    3. **ê¶Œì¥ì‚¬í•­ ë°˜ì˜**
    {self._format_recommendations(recommendations)}

    ### ğŸ“ ê°œì„  ì›ì¹™

    - ëª¨í˜¸í•œ í‘œí˜„ ì œê±°: "ì ì ˆíˆ", "ì¶©ë¶„íˆ" â†’ êµ¬ì²´ì  ê¸°ì¤€
    - ì¸¡ì • ê°€ëŠ¥ì„±: ëª¨ë“  ìš”êµ¬ì‚¬í•­ì— ì •ëŸ‰ì  ê¸°ì¤€ í¬í•¨
    - acceptance_criteria: ìµœì†Œ 3ê°œ ì´ìƒ, ê°ê° ê²€ì¦ ê°€ëŠ¥
    - ì„¸ë¶„í™”: í•˜ë‚˜ì˜ ìš”êµ¬ì‚¬í•­ = í•˜ë‚˜ì˜ ê¸°ëŠ¥

    ---

    ## ğŸ“„ ì›ë¬¸ ë¬¸ì„œ
    {text[:6000]}

    ---

    ìœ„ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ìš”êµ¬ì‚¬í•­ì„ ê°œì„ í•˜ì„¸ìš”.
    ê¸°ì¡´ ìš”êµ¬ì‚¬í•­ì€ ìœ ì§€í•˜ë˜, ë¬¸ì œê°€ ìˆëŠ” ë¶€ë¶„ì€ ìˆ˜ì •í•˜ê³ , ëˆ„ë½ëœ ë¶€ë¶„ì€ ì¶”ê°€í•˜ì„¸ìš”.

    ì¶œë ¥ JSON:
    {{{{
    "requirements": [
        {{{{
        "req_id": "REQ-001",
        "title": "...",
        "type": "functional",
        "priority": "High",
        "description": "...",
        "source_span": "...",
        "acceptance_criteria": [...]
        }}}}
    ]
    }}}}
    """
        
        # LLM í˜¸ì¶œ
        try:
            messages = [
                {"role": "system", "content": "You are a PM analyst expert in requirements refinement."},
                {"role": "user", "content": refinement_prompt}
            ]
            
            logger.info("[SCOPE] LLM ì¬ì¶”ì¶œ í˜¸ì¶œ")
            resp = self.llm.invoke(messages)
            content = _safe_extract_raw(resp)
            
            logger.info(f"[SCOPE] ì‘ë‹µ ê¸¸ì´: {len(content)}")
            
            # JSON íŒŒì‹±
            json_text = _json_first(content)
            if not json_text:
                logger.warning("[SCOPE] JSON ì¶”ì¶œ ì‹¤íŒ¨, ì´ì „ ê²°ê³¼ ë°˜í™˜")
                return previous_result
            
            result = _postprocess(json_text, text)
            
            logger.info(f"[SCOPE] ì¬ì¶”ì¶œ ì™„ë£Œ: {len(result)}ê°œ ìš”êµ¬ì‚¬í•­")
            
            return result
            
        except Exception as e:
            logger.error(f"[SCOPE] ì¬ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return previous_result


    def _build_feedback_section(self, score, issues, missing, recommendations):
        """í”¼ë“œë°± ì„¹ì…˜ ìƒì„±"""
        
        sections = []
        
        # ì ìˆ˜ ë° ë“±ê¸‰
        if score < 60:
            grade = "Poor"
            emoji = "âŒ"
        elif score < 75:
            grade = "Fair"
            emoji = "âš ï¸"
        elif score < 90:
            grade = "Good"
            emoji = "âœ…"
        else:
            grade = "Excellent"
            emoji = "ğŸŒŸ"
        
        sections.append(f"{emoji} í’ˆì§ˆ ë“±ê¸‰: {grade} ({score}/100)")
        
        # ì´ìŠˆ
        if issues:
            sections.append(f"\n**ë°œê²¬ëœ ì´ìŠˆ ({len(issues)}ê°œ):**")
            for i, issue in enumerate(issues[:10], 1):
                sections.append(f"{i}. {issue}")
        
        # ëˆ„ë½
        if missing:
            sections.append(f"\n**ëˆ„ë½ëœ ìš”êµ¬ì‚¬í•­ ({len(missing)}ê°œ):**")
            for i, miss in enumerate(missing[:5], 1):
                sections.append(f"{i}. {miss}")
        
        # ê¶Œì¥ì‚¬í•­
        if recommendations:
            sections.append(f"\n**ê°œì„  ê¶Œì¥ì‚¬í•­ ({len(recommendations)}ê°œ):**")
            for i, rec in enumerate(recommendations[:5], 1):
                sections.append(f"{i}. {rec}")
        
        return "\n".join(sections)


    def _summarize_requirements(self, requirements):
        """ìš”êµ¬ì‚¬í•­ ìš”ì•½"""
        
        if not requirements:
            return "ì—†ìŒ"
        
        summary = []
        for req in requirements[:10]:
            summary.append(
                f"- {req.get('req_id')}: {req.get('title')} "
                f"({req.get('type')}, {req.get('priority')})"
            )
        
        if len(requirements) > 10:
            summary.append(f"... ì™¸ {len(requirements) - 10}ê°œ")
        
        return "\n".join(summary)


    def _format_missing(self, missing):
        """ëˆ„ë½ í•­ëª© í¬ë§·íŒ…"""
        
        if not missing:
            return "ì—†ìŒ"
        
        formatted = []
        for i, miss in enumerate(missing, 1):
            formatted.append(f"   {i}. {miss}")
        
        return "\n".join(formatted)


    def _format_issues(self, issues):
        """ì´ìŠˆ í¬ë§·íŒ…"""
        
        if not issues:
            return "ì—†ìŒ"
        
        formatted = []
        for i, issue in enumerate(issues[:10], 1):
            formatted.append(f"   {i}. {issue}")
        
        return "\n".join(formatted)


    def _format_recommendations(self, recommendations):
        """ê¶Œì¥ì‚¬í•­ í¬ë§·íŒ…"""
        
        if not recommendations:
            return "ì—†ìŒ"
        
        formatted = []
        for i, rec in enumerate(recommendations[:5], 1):
            formatted.append(f"   {i}. {rec}")
        
        return "\n".join(formatted)


    # ============================================================================
    # extract_with_validation í•¨ìˆ˜ ê°œì„  ë²„ì „
    # ============================================================================

    def extract_with_validation_v2(scope_agent, 
                                quality_agent,
                                text: str, 
                                project_meta: Optional[Dict] = None,
                                max_attempts: int = 3,
                                strategy: str = "auto") -> Dict[str, Any]:
        """
        ê²€ì¦ ê¸°ë°˜ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ (ê°œì„  ë²„ì „)
        
        Args:
            scope_agent: ScopeAgent ì¸ìŠ¤í„´ìŠ¤
            quality_agent: QualityAgent ì¸ìŠ¤í„´ìŠ¤
            text: ì›ë³¸ ë¬¸ì„œ
            project_meta: í”„ë¡œì íŠ¸ ë©”íƒ€ë°ì´í„°
            max_attempts: ìµœëŒ€ ì‹œë„ íšŸìˆ˜
            strategy: ì¬ì¶”ì¶œ ì „ëµ ("auto", "feedback", "staged", "examples")
        
        Returns:
            {
                "success": bool,
                "requirements": [...],
                "validation": {...},
                "attempts": int,
                "improvements": [...],
                "history": [...]
            }
        """
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸš€ ê²€ì¦ ê¸°ë°˜ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ (ê°œì„  ë²„ì „)")
        logger.info(f"ğŸ“ ë¬¸ì„œ ê¸¸ì´: {len(text)} ë¬¸ì")
        logger.info(f"ğŸ”„ ìµœëŒ€ ì‹œë„: {max_attempts}íšŒ")
        logger.info(f"ğŸ“‹ ì „ëµ: {strategy}")
        logger.info(f"{'='*70}\n")
        
        history = []
        improvements = []
        previous_result = None
        validation_result = None
        
        for attempt in range(1, max_attempts + 1):
            logger.info(f"\n{'â”€'*70}")
            logger.info(f"ğŸ“ ì‹œë„ #{attempt}/{max_attempts}")
            logger.info(f"{'â”€'*70}")
            
            # 1. ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
            if attempt == 1:
                # ì²« ì‹œë„: ì¼ë°˜ ì¶”ì¶œ
                logger.info("ğŸ” ì´ˆê¸° ì¶”ì¶œ ìˆ˜í–‰")
                result = scope_agent.analyze_rfp(text, project_meta)
                method = "initial"
            else:
                # ì¬ì‹œë„: í”¼ë“œë°± ë°˜ì˜ ì¬ì¶”ì¶œ
                logger.info(f"ğŸ”„ í”¼ë“œë°± ê¸°ë°˜ ì¬ì¶”ì¶œ ìˆ˜í–‰")
                logger.info(f"   ì´ì „ ì ìˆ˜: {validation_result['score']:.1f}")
                logger.info(f"   ì´ìŠˆ: {len(validation_result['issues'])}ê°œ")
                logger.info(f"   ëˆ„ë½: {len(validation_result['missing_requirements'])}ê°œ")
                
                # refine_requirements ë©”ì„œë“œ í˜¸ì¶œ
                result = scope_agent.refine_requirements(
                    text, 
                    previous_result, 
                    validation_result,
                    project_meta
                )
                method = "refined"
            
            # ê²°ê³¼ ì •ê·œí™”
            requirements = result if isinstance(result, list) else result.get('requirements', [])
            logger.info(f"âœ… {len(requirements)}ê°œ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ ì™„ë£Œ")
            
            # 2. í’ˆì§ˆ ê²€ì¦
            logger.info("ğŸ” í’ˆì§ˆ ê²€ì¦ ì‹œì‘")
            validation_result = quality_agent.validate(requirements, text, project_meta)
            
            current_score = validation_result['score']
            logger.info(f"ğŸ¯ ì ìˆ˜: {current_score:.1f} ({validation_result['grade']})")
            
            # ê°œì„ ë„ ê³„ì‚°
            if previous_result:
                prev_score = history[-1]['validation']['score']
                improvement = current_score - prev_score
                improvements.append(improvement)
                logger.info(f"ğŸ“ˆ ê°œì„ ë„: {improvement:+.1f}ì ")
            
            # íˆìŠ¤í† ë¦¬ ì €ì¥
            history.append({
                "attempt": attempt,
                "method": method,
                "requirements_count": len(requirements),
                "validation": validation_result,
                "timestamp": datetime.now().isoformat()
            })
            
            # 3. í†µê³¼ ì—¬ë¶€ í™•ì¸
            if validation_result['pass']:
                logger.info(f"\n{'='*70}")
                logger.info(f"âœ… ê²€ì¦ í†µê³¼! (ì‹œë„ {attempt}íšŒ)")
                logger.info(f"ğŸ¯ ìµœì¢… ì ìˆ˜: {current_score:.1f}")
                logger.info(f"ğŸ† ë“±ê¸‰: {validation_result['grade']}")
                
                if attempt > 1:
                    total_improvement = current_score - history[0]['validation']['score']
                    logger.info(f"ğŸ“ˆ ì´ ê°œì„ : {total_improvement:+.1f}ì ")
                
                logger.info(f"{'='*70}\n")
                
                return {
                    "success": True,
                    "requirements": requirements,
                    "validation": validation_result,
                    "attempts": attempt,
                    "method": method,
                    "improvements": improvements,
                    "history": history,
                    "message": f"ê²€ì¦ í†µê³¼ (ì‹œë„ {attempt}íšŒ, ìµœì¢… ì ìˆ˜ {current_score:.1f})"
                }
            
            # ê²€ì¦ ì‹¤íŒ¨
            logger.warning(f"\nâš ï¸ ê²€ì¦ ë¯¸í†µê³¼")
            logger.warning(f"   ì ìˆ˜: {current_score:.1f} (ê¸°ì¤€: {quality_agent.threshold})")
            logger.warning(f"   ë“±ê¸‰: {validation_result['grade']}")
            
            if validation_result['issues']:
                logger.warning(f"\nğŸ“‹ ì£¼ìš” ì´ìŠˆ:")
                for i, issue in enumerate(validation_result['issues'][:5], 1):
                    logger.warning(f"   {i}. {issue}")
            
            if validation_result['missing_requirements']:
                logger.warning(f"\nğŸ“‹ ëˆ„ë½ í•­ëª©:")
                for i, miss in enumerate(validation_result['missing_requirements'][:3], 1):
                    logger.warning(f"   {i}. {miss}")
            
            # ë§ˆì§€ë§‰ ì‹œë„ ì „ ê²½ê³ 
            if attempt == max_attempts - 1:
                logger.warning(f"\nâš ï¸ ë§ˆì§€ë§‰ ì‹œë„ ì „ì…ë‹ˆë‹¤!")
            
            previous_result = requirements
        
        # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
        final_score = validation_result['score']
        logger.error(f"\n{'='*70}")
        logger.error(f"âŒ {max_attempts}íšŒ ì‹œë„ í›„ì—ë„ ê²€ì¦ ê¸°ì¤€ ë¯¸ë‹¬")
        logger.error(f"   ìµœì¢… ì ìˆ˜: {final_score:.1f} (ê¸°ì¤€: {quality_agent.threshold})")
        logger.error(f"   ìµœì¢… ë“±ê¸‰: {validation_result['grade']}")
        
        if improvements:
            avg_improvement = sum(improvements) / len(improvements)
            logger.error(f"   í‰ê·  ê°œì„ : {avg_improvement:+.1f}ì /ì‹œë„")
        
        logger.error(f"{'='*70}\n")
        
        return {
            "success": False,
            "requirements": requirements,
            "validation": validation_result,
            "attempts": max_attempts,
            "method": method,
            "improvements": improvements,
            "history": history,
            "message": f"í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ (ìµœì¢… {final_score:.1f}ì )"
        }


