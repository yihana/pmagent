from __future__ import annotations
import os
import re
import json
import asyncio
import time
import logging
import traceback
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

logger = logging.getLogger("scope.agent")

# prompts import (fallbacks provided)
try:
    from .prompts import SCOPE_EXTRACT_PROMPT, RTM_PROMPT, WBS_SYNTHESIS_PROMPT
except Exception:
    logger.warning("[SCOPE_AGENT] prompts import failed, using fallback prompts.")
    
    # ê°œì„ ëœ fallback prompt - ì„¸ë¶„í™” ê°•ì¡°
    SCOPE_EXTRACT_PROMPT = """
ë‹¹ì‹ ì€ PMP í‘œì¤€ì„ ì¤€ìˆ˜í•˜ëŠ” ì „ë¬¸ PMO ë¶„ì„ê°€ì…ë‹ˆë‹¤.

âš ï¸ ì¤‘ìš”: ê° ìš”êµ¬ì‚¬í•­ì€ **ë…ë¦½ì ìœ¼ë¡œ êµ¬í˜„ ê°€ëŠ¥í•œ ë‹¨ìœ„**ë¡œ ì„¸ë¶„í™”í•˜ì„¸ìš”.

## ì„¸ë¶„í™” ì›ì¹™
âœ… í•˜ë‚˜ì˜ ìš”êµ¬ì‚¬í•­ = í•˜ë‚˜ì˜ ê¸°ëŠ¥/íŠ¹ì„±
âœ… "~ë¥¼ í¬í•¨í•œë‹¤" í˜•íƒœì˜ ê·¸ë£¹í™” ê¸ˆì§€
âœ… ê° ìš”êµ¬ì‚¬í•­ì´ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•´ì•¼ í•¨

ë‚˜ìœ ì˜ˆ: "ì‚¬ìš©ì ê´€ë¦¬ ê¸°ëŠ¥ (íšŒì›ê°€ì…, ë¡œê·¸ì¸, í”„ë¡œí•„ í¬í•¨)"
ì¢‹ì€ ì˜ˆ: 
- REQ-001: ì´ë©”ì¼ ê¸°ë°˜ íšŒì›ê°€ì…
- REQ-002: ì†Œì…œ ë¡œê·¸ì¸ í†µí•©
- REQ-003: ì‚¬ìš©ì í”„ë¡œí•„ ê´€ë¦¬

## ì¶œë ¥ JSON êµ¬ì¡°
{{
  "requirements": [
    {{
      "req_id": "REQ-001",
      "title": "êµ¬ì²´ì ì¸ ê¸°ëŠ¥ëª… (20ì ì´ë‚´)",
      "type": "functional",
      "priority": "High",
      "description": "ë¬´ì—‡ì„ ì–´ë–»ê²Œ í•´ì•¼ í•˜ëŠ”ì§€ ìƒì„¸íˆ ê¸°ìˆ  (1-2ë¬¸ì¥)",
      "source_span": "ë¬¸ì„œ ì„¹ì…˜ ë²ˆí˜¸",
      "acceptance_criteria": [
        "ê²€ì¦ ê°€ëŠ¥í•œ ê¸°ì¤€ 1",
        "ê²€ì¦ ê°€ëŠ¥í•œ ê¸°ì¤€ 2"
      ]
    }}
  ],
  "functions": [],
  "deliverables": [],
  "acceptance_criteria": []
}}

ë¬¸ì„œ:
{context}
"""
    
    RTM_PROMPT = """
ìš”êµ¬ì‚¬í•­ ì¶”ì í‘œ(RTM) ìƒì„±:
{{requirements}}

ê° ìš”êµ¬ì‚¬í•­ì„ ì„¤ê³„-êµ¬í˜„-í…ŒìŠ¤íŠ¸ì™€ ë§¤í•‘í•˜ì„¸ìš”.
JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜.
"""
    
    WBS_SYNTHESIS_PROMPT = """
âš ï¸ WBSëŠ” Schedule Agentì—ì„œ ìƒì„±í•©ë‹ˆë‹¤.
Scope AgentëŠ” Requirements ì¶”ì¶œë§Œ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

# DB imports (optional)
try:
    from server.db.database import SessionLocal
    from server.db import pm_models
    _DB_AVAILABLE = True
except Exception as e:
    logger.warning("[ScopeAgent] DB import failed: %s", e)
    SessionLocal = None
    pm_models = None
    _DB_AVAILABLE = False

# LLM getter (uses server.utils.config.get_llm if present)
def get_llm():
    try:
        from server.utils.config import get_llm as _g
        llm = _g()
        logger.debug("[SCOPE_AGENT] get_llm() success: %s", getattr(llm, "__class__", llm))
        return llm
    except Exception as e:
        logger.warning("[SCOPE_AGENT] get_llm failed: %s", e)
        return None

# ------- Helpers -------

def _safe_extract_raw(resp: Any) -> str:
    """LLM ì‘ë‹µì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ì¶”ì¶œ(ìœ ì—° ì²˜ë¦¬)."""
    try:
        if resp is None:
            return ""
        
        # ì´ë¯¸ ë¬¸ìì—´ì¸ ê²½ìš° ë°”ë¡œ ë°˜í™˜ (ê°€ì¥ í”í•œ ì¼€ì´ìŠ¤)
        if isinstance(resp, str):
            logger.debug("[SCOPE] LLM ì‘ë‹µì´ ì´ë¯¸ ë¬¸ìì—´ì…ë‹ˆë‹¤")
            return resp
        
        # langchain/chat model-like: resp.generations / resp.generations[0].message.content
        if hasattr(resp, "generations"):
            gens = getattr(resp, "generations")
            try:
                # try to flatten common shapes
                if isinstance(gens, list) and len(gens) and hasattr(gens[0], "message"):
                    content = gens[0].message.content
                    logger.debug("[SCOPE] LLM ì‘ë‹µ ì¶”ì¶œ: generations[0].message.content")
                    return content
            except Exception:
                pass
        
        # Azure / OpenAI-like: resp.choices[0].message.content or resp.choices[0].text
        if hasattr(resp, "choices"):
            c = resp.choices
            if isinstance(c, (list, tuple)) and len(c):
                first = c[0]
                if hasattr(first, "message"):
                    if hasattr(first.message, "get"):
                        content = first.message.get("content", "")
                    elif hasattr(first.message, "content"):
                        content = first.message.content
                    else:
                        content = str(first.message)
                    logger.debug("[SCOPE] LLM ì‘ë‹µ ì¶”ì¶œ: choices[0].message")
                    return content
                if hasattr(first, "text"):
                    content = getattr(first, "text", "")
                    logger.debug("[SCOPE] LLM ì‘ë‹µ ì¶”ì¶œ: choices[0].text")
                    return content
        
        # some SDKs use .content directly
        if hasattr(resp, "content"):
            content = getattr(resp, "content")
            logger.debug("[SCOPE] LLM ì‘ë‹µ ì¶”ì¶œ: .content ì†ì„±")
            return content if isinstance(content, str) else str(content)
        
        # fallback to string conversion
        result = str(resp)
        logger.debug("[SCOPE] LLM ì‘ë‹µ ì¶”ì¶œ: str() ë³€í™˜")
        return result
    except Exception as e:
        logger.warning("[SCOPE] raw extract failed: %s", e)
        return str(resp) if resp else ""

def _json_from_text(maybe: str) -> Optional[dict]:
    """ë¬¸ìì—´ì—ì„œ ìµœì´ˆ JSON ê°ì²´(ì¤‘ê´„í˜¸)ë¥¼ ì¶”ì¶œí•´ íŒŒì‹± ì‹œë„."""
    if not maybe:
        return None
    try:
        # attempt to find JSON object, prefer full content if it's JSON
        s = maybe.strip()
        if s.startswith("{") and s.endswith("}"):
            return json.loads(s)
        m = re.search(r"(\{[\s\S]*\})", maybe)
        if m:
            return json.loads(m.group(1))
    except Exception as e:
        logger.debug("[SCOPE] json parse failed: %s", e)
    return None

def _estimate_confidence(resp_json: Optional[dict], raw_text: str) -> float:
    """
    ê°„ë‹¨í•œ confidence ì¶”ì •ê¸°:
    - LLMì´ 'confidence' í‚¤(0..1)ë¥¼ ë°˜í™˜í•˜ë©´ ìš°ì„  ì‚¬ìš©
    - ì•„ë‹ˆë©´ ìš”êµ¬ì‚¬í•­/ê¸°ëŠ¥ ìˆ˜, ê° ìš”êµ¬ì‚¬í•­ì˜ í•„ë“œ ì™„ì „ì„± ë“±ì„ ê¸°ì¤€ìœ¼ë¡œ 0..1 ì¶”ì •
    """
    if resp_json and isinstance(resp_json, dict):
        # direct provided confidence
        if "confidence" in resp_json:
            try:
                c = float(resp_json["confidence"])
                return min(max(c, 0.0), 1.0)
            except Exception:
                pass
        # heuristic: presence of requirements and fields
        reqs = resp_json.get("requirements") if resp_json else None
        if reqs and isinstance(reqs, list) and len(reqs) > 0:
            score = 0.5
            filled = 0
            for r in reqs:
                if r.get("req_id") and r.get("title") and r.get("description"):
                    filled += 1
            ratio = filled / len(reqs)
            score += 0.5 * ratio  # 0.5 ~ 1.0
            return min(score, 0.99)
    # fallback: if raw_text length small -> low confidence, else medium
    if raw_text and len(raw_text) > 800:
        return 0.6
    if raw_text and len(raw_text) > 200:
        return 0.45
    return 0.2

def _ensure_req_ids(reqs: List[dict]) -> List[dict]:
    """req_idê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±"""
    out = []
    ts = int(time.time())
    counter = 0
    for r in reqs:
        if not r.get("req_id"):
            counter += 1
            r["req_id"] = f"REQ-{ts}-{counter:02}"
        out.append(r)
    return out

# ------- ScopeAgent -------

class ScopeAgent:
    """RFP ë¬¸ì„œë¡œë¶€í„° Requirements/SRS/RTM/WBS(ì´ˆì•ˆ) ë“±ì„ ìƒì„±í•˜ëŠ” Agent
       ì¶”ê°€ ì˜µì…˜ (payload['options']):
         - confidence_threshold: float (0..1), default=0.75
         - max_attempts: int, default=3
    """

    def __init__(self, data_dir: Optional[str] = None):
        self.llm = get_llm()
        self.data_dir = data_dir or "data"
        logger.info(f"[SCOPE_AGENT] ì´ˆê¸°í™” ì™„ë£Œ - data_dir: {self.data_dir}")

    async def pipeline(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        project_id = payload.get("project_id") or payload.get("project_name") or "Unknown"
        text = payload.get("text") or ""
        documents = payload.get("documents") or []
        options = payload.get("options") or {}
        confidence_threshold = float(options.get("confidence_threshold", 0.75))
        max_attempts = int(options.get("max_attempts", 3))

        # if no text but documents - read first file path if exists (best-effort)
        if not text and documents:
            first = documents[0]
            path = None
            if isinstance(first, dict):
                path = first.get("path")
            else:
                path = getattr(first, "path", None)
            if path:
                p = Path(path)
                if not p.exists():
                    # maybe relative to data/inputs/RFP
                    alt = Path("data/inputs/RFP") / Path(path).name
                    if alt.exists():
                        p = alt
                if p.exists():
                    try:
                        # simple read for txt; for docx/pdf we expect upper layer to convert
                        text = p.read_text(encoding="utf-8", errors="ignore")
                    except Exception:
                        text = ""

        logger.info("ğŸ”µ [SCOPE] ìš”ì²­: project_id=%s, methodology=%s", project_id, payload.get("methodology"))

        # Run extraction with confidence loop
        items, raw_resp = await self._extract_items_with_confidence(text, confidence_threshold, max_attempts)

        # Ensure req_ids
        reqs = items.get("requirements", [])
        if reqs:
            items["requirements"] = _ensure_req_ids(reqs)

        # Write outputs: srs, scope md, rtm csv, wbs json draft
        out_dir = Path("data/outputs/scope") / str(project_id)
        out_dir.mkdir(parents=True, exist_ok=True)

        srs_path = out_dir / f"{project_id}_SRS.md"
        self._generate_srs(project_id, items, srs_path)

        # WBS draft: keep simple hierarchical draft (could be improved via WBS synthesis step)
        wbs = await self._synthesize_wbs_draft(items, depth=int(options.get("wbs_depth", 3)))
        wbs_path = out_dir / "wbs_structure.json"
        wbs_path.write_text(json.dumps(wbs, ensure_ascii=False, indent=2), encoding="utf-8")

        # RTM csv
        rtm_csv = out_dir / "rtm.csv"
        with rtm_csv.open("w", encoding="utf-8", newline="") as fh:
            fh.write("req_id,wbs_id,test_case,verification_status\n")
            for r in items.get("requirements", []):
                fh.write(f"{r.get('req_id')},,,Candidate\n")

        # attempt DB save (best-effort)
        saved = 0
        if _DB_AVAILABLE:
            try:
                saved = self._save_requirements_db(project_id, items)
            except Exception as e:
                logger.exception("[SCOPE] DB save failed: %s", e)
                saved = 0
        else:
            logger.debug("[SCOPE] DB not available, skipping DB save")

        # PMP outputs (scope_statement excel etc.) - keep existing hooks
        pmp_outputs = self._generate_pmp_outputs(project_id, out_dir, items)

        result = {
            "status": "ok",
            "project_id": project_id,
            "requirements": items.get("requirements", []),
            "functions": items.get("functions", []),
            "wbs_json": str(wbs_path),
            "wbs": wbs,
            "rtm_csv": str(rtm_csv),
            "srs_path": str(srs_path),
            "pmp_outputs": pmp_outputs,
            "db_saved_requirements": saved,
            "_llm_raw_response": str(raw_resp)[:2000],
        }
        logger.info("âœ… [SCOPE] ì‘ë‹µì™„ë£Œ: %s (requirements=%d, saved=%d)", project_id, len(items.get("requirements", [])), saved)
        return result

    async def _extract_items_with_confidence(self, text: str, threshold: float, max_attempts: int):
        """
        LLMì„ ë°˜ë³µ í˜¸ì¶œí•˜ì—¬ confidenceê°€ threshold ì´ìƒì¼ ë•Œê¹Œì§€ ì¬ì‹œë„.
        ë°˜í™˜: (items_dict, raw_response)
        """
        if not text:
            return {"requirements": [], "functions": []}, None

        llm = self.llm
        attempt = 0
        prev_raw = None
        last_items = None
        last_raw = None

        while attempt < max_attempts:
            attempt += 1
            logger.info("ğŸ”µ [SCOPE] LLM ì‹œë„ #%d (threshold=%.2f)", attempt, threshold)

            # build prompt - include previous output for refinement if present
            if last_items is None:
                prompt = SCOPE_EXTRACT_PROMPT.format(context=text[:8000])
            else:
                # refinement prompt: ask to improve/clarify previous JSON
                prompt = (
                    "ì´ì „ ì¶œë ¥ì„ ê°œì„ í•˜ì„¸ìš”. ì´ì „ ì¶œë ¥(JSON):\n"
                    f"{json.dumps(last_items, ensure_ascii=False, indent=2)}\n\n"
                    "ì›ë¬¸ ë¬¸ì„œ:\n"
                    f"{text[:4000]}\n\n"
                    "ìš”ì²­: ëˆ„ë½/ì¤‘ë³µ/ì˜ëª» ë§¤í•‘ëœ ìš”êµ¬ì‚¬í•­ì„ ìˆ˜ì •í•˜ê³ , ê° ìš”êµ¬ì‚¬í•­ì— req_id/title/description/type/priority/source_spanì„ ì œê³µí•˜ì„¸ìš”. "
                    "ìµœì¢… ê²°ê³¼ëŠ” JSONìœ¼ë¡œë§Œ ë°˜í™˜í•˜ì„¸ìš”."
                )

            raw_resp = None
            parsed = None
            try:
                if llm:
                    logger.info(f"ğŸ¤– [SCOPE] LLM í˜¸ì¶œ ì‹œì‘3 (í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)})")
                    
                    # LLM í˜¸ì¶œ - ë©”ì‹œì§€ í˜•ì‹ ìš°ì„  ì‹œë„
                    def call():
                        try:
                            # 1) ë©”ì‹œì§€ ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ë¨¼ì € ì‹œë„ (ê¶Œì¥)
                            if hasattr(llm, "invoke"):
                                logger.debug("[SCOPE] LLM í˜¸ì¶œ: invoke() ë©”ì„œë“œ - ë©”ì‹œì§€ í˜•ì‹")
                                messages = [
                                    {"role": "system", "content": "You are a PM analyst assistant."},
                                    {"role": "user", "content": prompt}
                                ]
                                return llm.invoke(messages)
                            
                            # 2) generate ë©”ì„œë“œ
                            if hasattr(llm, "generate"):
                                logger.debug("[SCOPE] LLM í˜¸ì¶œ: generate() ë©”ì„œë“œ")
                                return llm.generate(prompt)
                            
                            # 3) callableë¡œ ì§ì ‘ í˜¸ì¶œ
                            if callable(llm):
                                logger.debug("[SCOPE] LLM í˜¸ì¶œ: callable ì§ì ‘ í˜¸ì¶œ")
                                return llm(prompt)
                            
                            logger.warning("[SCOPE] LLM í˜¸ì¶œ ë°©ë²•ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                            return None
                        except Exception as e:
                            logger.error(f"[SCOPE] LLM í˜¸ì¶œ ì¤‘ ì˜ˆì™¸: {e}")
                            raise

                    resp = await asyncio.to_thread(call)
                    logger.info(f"âœ… [SCOPE] LLM ì‘ë‹µ ìˆ˜ì‹  (íƒ€ì…: {type(resp).__name__})")
                    
                    # ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    raw_resp = _safe_extract_raw(resp)
                    logger.info(f"ğŸ“„ [SCOPE] ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ (ê¸¸ì´: {len(str(raw_resp))})")
                    logger.debug(f"[SCOPE] ì‘ë‹µ ë‚´ìš© (ì²˜ìŒ 500ì):\n{str(raw_resp)[:500]}")
                else:
                    logger.warning("[SCOPE] LLM ë¯¸ì„¤ì •, fallback ì‚¬ìš©")
                    return self._fallback_extract(text), None
            except Exception as e:
                logger.warning("ğŸŸ  [SCOPE] LLM í˜¸ì¶œ ì‹¤íŒ¨: %s", e)
                logger.debug(f"[SCOPE] ì‹¤íŒ¨ ìƒì„¸:\n{traceback.format_exc()}")
                # fallback to rule extraction if first attempt fails
                if attempt == max_attempts:
                    return self._fallback_extract(text), None
                last_items = None
                last_raw = str(e)
                continue

            # try parse JSON from raw_resp
            parsed = _json_from_text(raw_resp)
            confidence = _estimate_confidence(parsed, raw_resp)
            logger.info("ğŸ”µ [SCOPE] parsed=%s, estimated_confidence=%.3f", bool(parsed), confidence)

            if parsed and confidence >= threshold:
                logger.info("âœ… [SCOPE] confidence threshold met (%.3f >= %.3f) on attempt %d", confidence, threshold, attempt)
                return parsed, raw_resp

            # If parsed but confidence low, set last_items to parsed and re-prompt for refinement
            if parsed:
                last_items = parsed
                last_raw = raw_resp
                # continue loop to refine
                logger.info("[SCOPE] ì¬ì‹œë„: parsed but low confidence (%.3f). ì¬í”„ë¡¬í”„íŠ¸ ì§„í–‰...", confidence)
                await asyncio.sleep(0.2)  # small backoff
                continue

            # If not parsed (no JSON), provide guidance and try again
            logger.info("[SCOPE] JSON íŒŒì‹± ì‹¤íŒ¨ í˜¹ì€ í¬ë§· ì˜¤ë¥˜ â€” ì¬ì‹œë„í•©ë‹ˆë‹¤ (attempt %d)", attempt)
            # create a clarifying prompt forcing JSON output
            last_items = None
            last_raw = raw_resp
            await asyncio.sleep(0.2)
            continue

        # after attempts exhausted, fallback to parsed if any, else rule-based
        if last_items:
            logger.warning("[SCOPE] ìµœëŒ€ ì‹œë„(%d) ë„ë‹¬: ë§ˆì§€ë§‰ parsed ì‚¬ìš© (confidence %.3f)", max_attempts, _estimate_confidence(last_items, last_raw))
            return last_items, last_raw
        logger.warning("[SCOPE] ìµœëŒ€ ì‹œë„(%d) ë„ë‹¬: fallback ê·œì¹™ ê¸°ë°˜ ì‚¬ìš©", max_attempts)
        return self._fallback_extract(text), last_raw

    def _fallback_extract(self, text: str) -> Dict[str, Any]:
        """ê°„ë‹¨ ê·œì¹™ ê¸°ë°˜ (ê¸°ì¡´ fallback ìœ ì§€)"""
        reqs = []
        funcs = []
        for i, ln in enumerate([l.strip() for l in text.splitlines() if l.strip()]):
            if re.search(r"(ìš”êµ¬|í•„ìš”|í•´ì•¼|shall|must|should)", ln, re.I):
                reqs.append({
                    "req_id": None,
                    "title": ln[:80],
                    "type": "functional",
                    "priority": "Medium",
                    "description": ln,
                    "source_span": f"line {i+1}"
                })
            if re.search(r"(ê¸°ëŠ¥|ê¸°ëŠ¥ëª…|support|provide)", ln, re.I):
                funcs.append({"name": ln[:80], "desc": ln})
        logger.info("âœ… [SCOPE] fallback ì¶”ì¶œ: %d reqs, %d funcs", len(reqs), len(funcs))
        return {"requirements": reqs, "functions": funcs}

    async def _synthesize_wbs_draft(self, items: Dict[str, Any], depth: int = 3) -> Dict[str, Any]:
        """ê°„ë‹¨í•œ WBS ì´ˆì•ˆ ìƒì„± (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        nodes = [{
            "id": "WBS-1",
            "name": "Project",
            "level": 1,
            "children": []
        }]
        phases = []
        reqs = items.get("requirements", [])
        # naive: split into phases of ~ceil(len(reqs)/3)
        if reqs:
            per = max(1, (len(reqs) + 2) // 3)
            for i in range(3):
                start = i * per
                seg = reqs[start:start+per]
                phase = {
                    "id": f"WBS-1.{i+1}",
                    "name": f"Phase {i+1}",
                    "level": 2,
                    "children": []
                }
                # tasks per requirement
                for j, r in enumerate(seg, 1):
                    phase["children"].append({
                        "id": f"{phase['id']}.{j}",
                        "name": r.get("title", f"Task {i+1}.{j}")[:60],
                        "level": 3,
                        "owner": None,
                        "deliverables": None
                    })
                phases.append(phase)
        else:
            # default phases
            for i in range(3):
                phases.append({
                    "id": f"WBS-1.{i+1}",
                    "name": f"Phase {i+1}",
                    "level": 2,
                    "children": []
                })
        nodes[0]["children"] = phases
        return {"nodes": nodes, "depth": depth}

    def _save_requirements_db(self, project_id: str, items: Dict[str, Any]) -> int:
        """
        DBì— ìš”êµ¬ì‚¬í•­ ì €ì¥(ê°„ë‹¨ êµ¬í˜„). ë°˜í™˜: ì €ì¥ëœ ë ˆì½”ë“œ ìˆ˜.
        ì•ˆì „ ì¥ì¹˜: req_idê°€ ë¹„ì–´ìˆìœ¼ë©´ ìë™ìƒì„± í›„ ì €ì¥.
        """
        if not _DB_AVAILABLE:
            logger.debug("[SCOPE] DB not available")
            return 0
        db = SessionLocal()
        saved = 0
        try:
            reqs = items.get("requirements", []) or []
            for r in reqs:
                req_id = r.get("req_id")
                title = r.get("title") or r.get("description")[:200]
                description = r.get("description")
                rtype = r.get("type") or r.get("category") or "functional"
                priority = r.get("priority") or "Medium"
                source_doc = r.get("source_span") or None

                if not req_id:
                    req_id = f"AUTO-{int(time.time())}-{saved+1}"
                    logger.debug("[SCOPE] ìë™ req_id ìƒì„±: %s", req_id)

                # upsert-like: try find existing by project_id + req_id
                existing = db.query(pm_models.PM_Requirement).filter_by(project_id=project_id, req_id=req_id).first()
                if existing:
                    existing.title = title
                    existing.description = description
                    existing.priority = priority
                    existing.type = rtype
                    existing.source_doc = source_doc
                else:
                    obj = pm_models.PM_Requirement(
                        project_id=project_id,
                        req_id=req_id,
                        title=title,
                        description=description,
                        priority=priority,
                        status="new",
                        source_doc=source_doc,
                        created_at=datetime.utcnow()
                    )
                    db.add(obj)
                saved += 1
            db.commit()
            logger.info("[SCOPE] DB ì €ì¥ ì™„ë£Œ: %d ë ˆì½”ë“œ", saved)
            return saved
        except Exception as e:
            logger.exception("[SCOPE] Saving to DB failed: %s", e)
            try:
                db.rollback()
            except Exception:
                pass
            return 0
        finally:
            db.close()

    # SRS / Charter / PMP outputs (existing hooks)
    def _generate_srs(self, project_id: Any, items: Dict[str, Any], out_path: Path):
        out_path.parent.mkdir(parents=True, exist_ok=True)
        with open(out_path, "w", encoding="utf-8") as f:
            f.write(f"# Software Requirements Specification\n")
            f.write(f"**Project:** {project_id}\n")
            f.write(f"**Generated:** {datetime.utcnow().isoformat()}\n\n")
            f.write("## 1. Requirements\n\n")
            for r in items.get("requirements", []):
                f.write(f"### {r.get('req_id')}: {r.get('title')}\n")
                f.write(f"- **Type:** {r.get('type')}\n")
                f.write(f"- **Priority:** {r.get('priority')}\n")
                f.write(f"- **Description:** {r.get('description')}\n")
                f.write(f"- **Source:** {r.get('source_span')}\n\n")
        return str(out_path)

    def _generate_pmp_outputs(self, project_id: Any, project_dir: Path, requirements: Dict[str, Any]) -> Dict[str, Optional[str]]:
        outputs = {}
        try:
            from .outputs.scope_statement import ScopeStatementGenerator
            scp = project_dir / f"{project_id}_ScopeStatement.xlsx"
            outputs["scope_statement_excel"] = ScopeStatementGenerator.generate(project_id, requirements, scp)
        except Exception as e:
            outputs["scope_statement_excel"] = None
            logger.debug("ScopeStatementGenerator not available: %s", e)
        return outputs
    
    # ScopeAgentì— ì¶”ê°€í•  ë©”ì„œë“œë“¤
    # pipeline.pyì˜ ScopeAgent í´ë˜ìŠ¤ì— ì¶”ê°€

    def refine_requirements(self, 
                        text: str, 
                        previous_result: List[Dict[str, Any]],
                        validation_result: Dict[str, Any],
                        project_meta: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        ê²€ì¦ í”¼ë“œë°±ì„ ë°˜ì˜í•œ ìš”êµ¬ì‚¬í•­ ì¬ì¶”ì¶œ
        
        Args:
            text: ì›ë³¸ ë¬¸ì„œ
            previous_result: ì´ì „ ì¶”ì¶œ ê²°ê³¼
            validation_result: í’ˆì§ˆ ê²€ì¦ ê²°ê³¼
            project_meta: í”„ë¡œì íŠ¸ ë©”íƒ€ë°ì´í„°
        
        Returns:
            ê°œì„ ëœ ìš”êµ¬ì‚¬í•­ ë¦¬ìŠ¤íŠ¸
        """
        logger.info("[SCOPE] í”¼ë“œë°± ê¸°ë°˜ ì¬ì¶”ì¶œ ì‹œì‘")
        
        # ê²€ì¦ ê²°ê³¼ ë¶„ì„
        score = validation_result.get('score', 0)
        issues = validation_result.get('issues', [])
        missing = validation_result.get('missing_requirements', [])
        recommendations = validation_result.get('recommendations', [])
        
        logger.info(f"[SCOPE] ì´ì „ ì ìˆ˜: {score}")
        logger.info(f"[SCOPE] ì´ìŠˆ: {len(issues)}ê°œ")
        logger.info(f"[SCOPE] ëˆ„ë½: {len(missing)}ê°œ")
        
        # í”¼ë“œë°± í”„ë¡¬í”„íŠ¸ ìƒì„±
        feedback_section = self._build_feedback_section(
            score, issues, missing, recommendations
        )
        
        # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
        refinement_prompt = f"""
    ë‹¹ì‹ ì€ PMP í‘œì¤€ì„ ì¤€ìˆ˜í•˜ëŠ” ì „ë¬¸ PMO ë¶„ì„ê°€ì…ë‹ˆë‹¤.

    ## ğŸ”„ ìš”êµ¬ì‚¬í•­ ê°œì„  ì‘ì—…

    ### ì´ì „ ì¶”ì¶œ ê²°ê³¼
    ì¶”ì¶œëœ ìš”êµ¬ì‚¬í•­: {len(previous_result)}ê°œ
    í’ˆì§ˆ ì ìˆ˜: {score}/100

    ### ğŸ“‹ ì´ì „ ì¶”ì¶œ ê²°ê³¼ (ìš”ì•½)
    {self._summarize_requirements(previous_result)}

    ### âš ï¸ ê²€ì¦ì—ì„œ ë°œê²¬ëœ ë¬¸ì œì 

    {feedback_section}

    ### ğŸ¯ ê°œì„  ì§€ì¹¨

    1. **ëˆ„ë½ëœ ìš”êµ¬ì‚¬í•­ ì¶”ê°€**
    {self._format_missing(missing)}

    2. **ê¸°ì¡´ ìš”êµ¬ì‚¬í•­ ê°œì„ **
    {self._format_issues(issues)}

    3. **ê¶Œì¥ì‚¬í•­ ë°˜ì˜**
    {self._format_recommendations(recommendations)}

    ### ğŸ“ ê°œì„  ì›ì¹™

    - ëª¨í˜¸í•œ í‘œí˜„ ì œê±°: "ì ì ˆíˆ", "ì¶©ë¶„íˆ" â†’ êµ¬ì²´ì  ê¸°ì¤€
    - ì¸¡ì • ê°€ëŠ¥ì„±: ëª¨ë“  ìš”êµ¬ì‚¬í•­ì— ì •ëŸ‰ì  ê¸°ì¤€ í¬í•¨
    - acceptance_criteria: ìµœì†Œ 3ê°œ ì´ìƒ, ê°ê° ê²€ì¦ ê°€ëŠ¥
    - ì„¸ë¶„í™”: í•˜ë‚˜ì˜ ìš”êµ¬ì‚¬í•­ = í•˜ë‚˜ì˜ ê¸°ëŠ¥

    ---

    ## ğŸ“„ ì›ë¬¸ ë¬¸ì„œ
    {text[:6000]}

    ---

    ìœ„ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ìš”êµ¬ì‚¬í•­ì„ ê°œì„ í•˜ì„¸ìš”.
    ê¸°ì¡´ ìš”êµ¬ì‚¬í•­ì€ ìœ ì§€í•˜ë˜, ë¬¸ì œê°€ ìˆëŠ” ë¶€ë¶„ì€ ìˆ˜ì •í•˜ê³ , ëˆ„ë½ëœ ë¶€ë¶„ì€ ì¶”ê°€í•˜ì„¸ìš”.

    ì¶œë ¥ JSON:
    {{{{
    "requirements": [
        {{{{
        "req_id": "REQ-001",
        "title": "...",
        "type": "functional",
        "priority": "High",
        "description": "...",
        "source_span": "...",
        "acceptance_criteria": [...]
        }}}}
    ]
    }}}}
    """
        
        # LLM í˜¸ì¶œ
        try:
            messages = [
                {"role": "system", "content": "You are a PM analyst expert in requirements refinement."},
                {"role": "user", "content": refinement_prompt}
            ]
            
            logger.info("[SCOPE] LLM ì¬ì¶”ì¶œ í˜¸ì¶œ")
            resp = self.llm.invoke(messages)
            content = _safe_extract_raw(resp)
            
            logger.info(f"[SCOPE] ì‘ë‹µ ê¸¸ì´: {len(content)}")
            
            # JSON íŒŒì‹±
            json_text = _json_first(content)
            if not json_text:
                logger.warning("[SCOPE] JSON ì¶”ì¶œ ì‹¤íŒ¨, ì´ì „ ê²°ê³¼ ë°˜í™˜")
                return previous_result
            
            result = _postprocess(json_text, text)
            
            logger.info(f"[SCOPE] ì¬ì¶”ì¶œ ì™„ë£Œ: {len(result)}ê°œ ìš”êµ¬ì‚¬í•­")
            
            return result
            
        except Exception as e:
            logger.error(f"[SCOPE] ì¬ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return previous_result


    def _build_feedback_section(self, score, issues, missing, recommendations):
        """í”¼ë“œë°± ì„¹ì…˜ ìƒì„±"""
        
        sections = []
        
        # ì ìˆ˜ ë° ë“±ê¸‰
        if score < 60:
            grade = "Poor"
            emoji = "âŒ"
        elif score < 75:
            grade = "Fair"
            emoji = "âš ï¸"
        elif score < 90:
            grade = "Good"
            emoji = "âœ…"
        else:
            grade = "Excellent"
            emoji = "ğŸŒŸ"
        
        sections.append(f"{emoji} í’ˆì§ˆ ë“±ê¸‰: {grade} ({score}/100)")
        
        # ì´ìŠˆ
        if issues:
            sections.append(f"\n**ë°œê²¬ëœ ì´ìŠˆ ({len(issues)}ê°œ):**")
            for i, issue in enumerate(issues[:10], 1):
                sections.append(f"{i}. {issue}")
        
        # ëˆ„ë½
        if missing:
            sections.append(f"\n**ëˆ„ë½ëœ ìš”êµ¬ì‚¬í•­ ({len(missing)}ê°œ):**")
            for i, miss in enumerate(missing[:5], 1):
                sections.append(f"{i}. {miss}")
        
        # ê¶Œì¥ì‚¬í•­
        if recommendations:
            sections.append(f"\n**ê°œì„  ê¶Œì¥ì‚¬í•­ ({len(recommendations)}ê°œ):**")
            for i, rec in enumerate(recommendations[:5], 1):
                sections.append(f"{i}. {rec}")
        
        return "\n".join(sections)


    def _summarize_requirements(self, requirements):
        """ìš”êµ¬ì‚¬í•­ ìš”ì•½"""
        
        if not requirements:
            return "ì—†ìŒ"
        
        summary = []
        for req in requirements[:10]:
            summary.append(
                f"- {req.get('req_id')}: {req.get('title')} "
                f"({req.get('type')}, {req.get('priority')})"
            )
        
        if len(requirements) > 10:
            summary.append(f"... ì™¸ {len(requirements) - 10}ê°œ")
        
        return "\n".join(summary)


    def _format_missing(self, missing):
        """ëˆ„ë½ í•­ëª© í¬ë§·íŒ…"""
        
        if not missing:
            return "ì—†ìŒ"
        
        formatted = []
        for i, miss in enumerate(missing, 1):
            formatted.append(f"   {i}. {miss}")
        
        return "\n".join(formatted)


    def _format_issues(self, issues):
        """ì´ìŠˆ í¬ë§·íŒ…"""
        
        if not issues:
            return "ì—†ìŒ"
        
        formatted = []
        for i, issue in enumerate(issues[:10], 1):
            formatted.append(f"   {i}. {issue}")
        
        return "\n".join(formatted)


    def _format_recommendations(self, recommendations):
        """ê¶Œì¥ì‚¬í•­ í¬ë§·íŒ…"""
        
        if not recommendations:
            return "ì—†ìŒ"
        
        formatted = []
        for i, rec in enumerate(recommendations[:5], 1):
            formatted.append(f"   {i}. {rec}")
        
        return "\n".join(formatted)


    # ============================================================================
    # extract_with_validation í•¨ìˆ˜ ê°œì„  ë²„ì „
    # ============================================================================

    def extract_with_validation_v2(scope_agent, 
                                quality_agent,
                                text: str, 
                                project_meta: Optional[Dict] = None,
                                max_attempts: int = 3,
                                strategy: str = "auto") -> Dict[str, Any]:
        """
        ê²€ì¦ ê¸°ë°˜ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ (ê°œì„  ë²„ì „)
        
        Args:
            scope_agent: ScopeAgent ì¸ìŠ¤í„´ìŠ¤
            quality_agent: QualityAgent ì¸ìŠ¤í„´ìŠ¤
            text: ì›ë³¸ ë¬¸ì„œ
            project_meta: í”„ë¡œì íŠ¸ ë©”íƒ€ë°ì´í„°
            max_attempts: ìµœëŒ€ ì‹œë„ íšŸìˆ˜
            strategy: ì¬ì¶”ì¶œ ì „ëµ ("auto", "feedback", "staged", "examples")
        
        Returns:
            {
                "success": bool,
                "requirements": [...],
                "validation": {...},
                "attempts": int,
                "improvements": [...],
                "history": [...]
            }
        """
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸš€ ê²€ì¦ ê¸°ë°˜ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ (ê°œì„  ë²„ì „)")
        logger.info(f"ğŸ“ ë¬¸ì„œ ê¸¸ì´: {len(text)} ë¬¸ì")
        logger.info(f"ğŸ”„ ìµœëŒ€ ì‹œë„: {max_attempts}íšŒ")
        logger.info(f"ğŸ“‹ ì „ëµ: {strategy}")
        logger.info(f"{'='*70}\n")
        
        history = []
        improvements = []
        previous_result = None
        validation_result = None
        
        for attempt in range(1, max_attempts + 1):
            logger.info(f"\n{'â”€'*70}")
            logger.info(f"ğŸ“ ì‹œë„ #{attempt}/{max_attempts}")
            logger.info(f"{'â”€'*70}")
            
            # 1. ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
            if attempt == 1:
                # ì²« ì‹œë„: ì¼ë°˜ ì¶”ì¶œ
                logger.info("ğŸ” ì´ˆê¸° ì¶”ì¶œ ìˆ˜í–‰")
                result = scope_agent.analyze_rfp(text, project_meta)
                method = "initial"
            else:
                # ì¬ì‹œë„: í”¼ë“œë°± ë°˜ì˜ ì¬ì¶”ì¶œ
                logger.info(f"ğŸ”„ í”¼ë“œë°± ê¸°ë°˜ ì¬ì¶”ì¶œ ìˆ˜í–‰")
                logger.info(f"   ì´ì „ ì ìˆ˜: {validation_result['score']:.1f}")
                logger.info(f"   ì´ìŠˆ: {len(validation_result['issues'])}ê°œ")
                logger.info(f"   ëˆ„ë½: {len(validation_result['missing_requirements'])}ê°œ")
                
                # refine_requirements ë©”ì„œë“œ í˜¸ì¶œ
                result = scope_agent.refine_requirements(
                    text, 
                    previous_result, 
                    validation_result,
                    project_meta
                )
                method = "refined"
            
            # ê²°ê³¼ ì •ê·œí™”
            requirements = result if isinstance(result, list) else result.get('requirements', [])
            logger.info(f"âœ… {len(requirements)}ê°œ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ ì™„ë£Œ")
            
            # 2. í’ˆì§ˆ ê²€ì¦
            logger.info("ğŸ” í’ˆì§ˆ ê²€ì¦ ì‹œì‘")
            validation_result = quality_agent.validate(requirements, text, project_meta)
            
            current_score = validation_result['score']
            logger.info(f"ğŸ¯ ì ìˆ˜: {current_score:.1f} ({validation_result['grade']})")
            
            # ê°œì„ ë„ ê³„ì‚°
            if previous_result:
                prev_score = history[-1]['validation']['score']
                improvement = current_score - prev_score
                improvements.append(improvement)
                logger.info(f"ğŸ“ˆ ê°œì„ ë„: {improvement:+.1f}ì ")
            
            # íˆìŠ¤í† ë¦¬ ì €ì¥
            history.append({
                "attempt": attempt,
                "method": method,
                "requirements_count": len(requirements),
                "validation": validation_result,
                "timestamp": datetime.now().isoformat()
            })
            
            # 3. í†µê³¼ ì—¬ë¶€ í™•ì¸
            if validation_result['pass']:
                logger.info(f"\n{'='*70}")
                logger.info(f"âœ… ê²€ì¦ í†µê³¼! (ì‹œë„ {attempt}íšŒ)")
                logger.info(f"ğŸ¯ ìµœì¢… ì ìˆ˜: {current_score:.1f}")
                logger.info(f"ğŸ† ë“±ê¸‰: {validation_result['grade']}")
                
                if attempt > 1:
                    total_improvement = current_score - history[0]['validation']['score']
                    logger.info(f"ğŸ“ˆ ì´ ê°œì„ : {total_improvement:+.1f}ì ")
                
                logger.info(f"{'='*70}\n")
                
                return {
                    "success": True,
                    "requirements": requirements,
                    "validation": validation_result,
                    "attempts": attempt,
                    "method": method,
                    "improvements": improvements,
                    "history": history,
                    "message": f"ê²€ì¦ í†µê³¼ (ì‹œë„ {attempt}íšŒ, ìµœì¢… ì ìˆ˜ {current_score:.1f})"
                }
            
            # ê²€ì¦ ì‹¤íŒ¨
            logger.warning(f"\nâš ï¸ ê²€ì¦ ë¯¸í†µê³¼")
            logger.warning(f"   ì ìˆ˜: {current_score:.1f} (ê¸°ì¤€: {quality_agent.threshold})")
            logger.warning(f"   ë“±ê¸‰: {validation_result['grade']}")
            
            if validation_result['issues']:
                logger.warning(f"\nğŸ“‹ ì£¼ìš” ì´ìŠˆ:")
                for i, issue in enumerate(validation_result['issues'][:5], 1):
                    logger.warning(f"   {i}. {issue}")
            
            if validation_result['missing_requirements']:
                logger.warning(f"\nğŸ“‹ ëˆ„ë½ í•­ëª©:")
                for i, miss in enumerate(validation_result['missing_requirements'][:3], 1):
                    logger.warning(f"   {i}. {miss}")
            
            # ë§ˆì§€ë§‰ ì‹œë„ ì „ ê²½ê³ 
            if attempt == max_attempts - 1:
                logger.warning(f"\nâš ï¸ ë§ˆì§€ë§‰ ì‹œë„ ì „ì…ë‹ˆë‹¤!")
            
            previous_result = requirements
        
        # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
        final_score = validation_result['score']
        logger.error(f"\n{'='*70}")
        logger.error(f"âŒ {max_attempts}íšŒ ì‹œë„ í›„ì—ë„ ê²€ì¦ ê¸°ì¤€ ë¯¸ë‹¬")
        logger.error(f"   ìµœì¢… ì ìˆ˜: {final_score:.1f} (ê¸°ì¤€: {quality_agent.threshold})")
        logger.error(f"   ìµœì¢… ë“±ê¸‰: {validation_result['grade']}")
        
        if improvements:
            avg_improvement = sum(improvements) / len(improvements)
            logger.error(f"   í‰ê·  ê°œì„ : {avg_improvement:+.1f}ì /ì‹œë„")
        
        logger.error(f"{'='*70}\n")
        
        return {
            "success": False,
            "requirements": requirements,
            "validation": validation_result,
            "attempts": max_attempts,
            "method": method,
            "improvements": improvements,
            "history": history,
            "message": f"í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ (ìµœì¢… {final_score:.1f}ì )"
        }
