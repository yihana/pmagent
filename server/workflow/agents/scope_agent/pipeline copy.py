from __future__ import annotations
import os
import re
import json
import asyncio
import time
import logging
import traceback
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

logger = logging.getLogger("scope.agent")

# prompts import (fallbacks provided)
try:
    from .prompts import SCOPE_EXTRACT_PROMPT, RTM_PROMPT, WBS_SYNTHESIS_PROMPT
except Exception:
    logger.warning("[SCOPE_AGENT] prompts import failed, using fallback prompts.")
    
    # ê°œì„ ëœ fallback prompt - ì„¸ë¶„í™” ê°•ì¡°
    SCOPE_EXTRACT_PROMPT = """
ë‹¹ì‹ ì€ PMP í‘œì¤€ì„ ì¤€ìˆ˜í•˜ëŠ” ì „ë¬¸ PMO ë¶„ì„ê°€ì…ë‹ˆë‹¤.

âš ï¸ ì¤‘ìš”: ê° ìš”êµ¬ì‚¬í•­ì€ **ë…ë¦½ì ìœ¼ë¡œ êµ¬í˜„ ê°€ëŠ¥í•œ ë‹¨ìœ„**ë¡œ ì„¸ë¶„í™”í•˜ì„¸ìš”.

## ì„¸ë¶„í™” ì›ì¹™
âœ… í•˜ë‚˜ì˜ ìš”êµ¬ì‚¬í•­ = í•˜ë‚˜ì˜ ê¸°ëŠ¥/íŠ¹ì„±
âœ… "~ë¥¼ í¬í•¨í•œë‹¤" í˜•íƒœì˜ ê·¸ë£¹í™” ê¸ˆì§€
âœ… ê° ìš”êµ¬ì‚¬í•­ì´ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•´ì•¼ í•¨

ë‚˜ìœ ì˜ˆ: "ì‚¬ìš©ì ê´€ë¦¬ ê¸°ëŠ¥ (íšŒì›ê°€ì…, ë¡œê·¸ì¸, í”„ë¡œí•„ í¬í•¨)"
ì¢‹ì€ ì˜ˆ: 
- REQ-001: ì´ë©”ì¼ ê¸°ë°˜ íšŒì›ê°€ì…
- REQ-002: ì†Œì…œ ë¡œê·¸ì¸ í†µí•©
- REQ-003: ì‚¬ìš©ì í”„ë¡œí•„ ê´€ë¦¬

## ì¶œë ¥ JSON êµ¬ì¡°
{{
  "requirements": [
    {{
      "req_id": "REQ-001",
      "title": "êµ¬ì²´ì ì¸ ê¸°ëŠ¥ëª… (20ì ì´ë‚´)",
      "type": "functional",
      "priority": "High",
      "description": "ë¬´ì—‡ì„ ì–´ë–»ê²Œ í•´ì•¼ í•˜ëŠ”ì§€ ìƒì„¸íˆ ê¸°ìˆ  (1-2ë¬¸ì¥)",
      "source_span": "ë¬¸ì„œ ì„¹ì…˜ ë²ˆí˜¸",
      "acceptance_criteria": [
        "ê²€ì¦ ê°€ëŠ¥í•œ ê¸°ì¤€ 1",
        "ê²€ì¦ ê°€ëŠ¥í•œ ê¸°ì¤€ 2"
      ]
    }}
  ],
  "functions": [],
  "deliverables": [],
  "acceptance_criteria": []
}}

ë¬¸ì„œ:
{context}
"""
    
    RTM_PROMPT = """
ìš”êµ¬ì‚¬í•­ ì¶”ì í‘œ(RTM) ìƒì„±:
{{requirements}}

ê° ìš”êµ¬ì‚¬í•­ì„ ì„¤ê³„-êµ¬í˜„-í…ŒìŠ¤íŠ¸ì™€ ë§¤í•‘í•˜ì„¸ìš”.
JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜.
"""
    
    WBS_SYNTHESIS_PROMPT = """
âš ï¸ WBSëŠ” Schedule Agentì—ì„œ ìƒì„±í•©ë‹ˆë‹¤.
Scope AgentëŠ” Requirements ì¶”ì¶œë§Œ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

# DB imports (optional)
try:
    from server.db.database import SessionLocal
    from server.db import pm_models
    _DB_AVAILABLE = True
except Exception as e:
    logger.warning("[ScopeAgent] DB import failed: %s", e)
    SessionLocal = None
    pm_models = None
    _DB_AVAILABLE = False

# LLM getter (uses server.utils.config.get_llm if present)
def get_llm():
    try:
        from server.utils.config import get_llm as _g
        llm = _g()
        logger.debug("[SCOPE_AGENT] get_llm() success: %s", getattr(llm, "__class__", llm))
        return llm
    except Exception as e:
        logger.warning("[SCOPE_AGENT] get_llm failed: %s", e)
        return None

# ------- Helpers -------

def _safe_extract_raw(resp: Any) -> str:
    """LLM ì‘ë‹µì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ì¶”ì¶œ(ìœ ì—° ì²˜ë¦¬)."""
    try:
        if resp is None:
            return ""
        
        # ì´ë¯¸ ë¬¸ìì—´ì¸ ê²½ìš° ë°”ë¡œ ë°˜í™˜ (ê°€ì¥ í”í•œ ì¼€ì´ìŠ¤)
        if isinstance(resp, str):
            logger.debug("[SCOPE] LLM ì‘ë‹µì´ ì´ë¯¸ ë¬¸ìì—´ì…ë‹ˆë‹¤")
            return resp
        
        # langchain/chat model-like: resp.generations / resp.generations[0].message.content
        if hasattr(resp, "generations"):
            gens = getattr(resp, "generations")
            try:
                # try to flatten common shapes
                if isinstance(gens, list) and len(gens) and hasattr(gens[0], "message"):
                    content = gens[0].message.content
                    logger.debug("[SCOPE] LLM ì‘ë‹µ ì¶”ì¶œ: generations[0].message.content")
                    return content
            except Exception:
                pass
        
        # Azure / OpenAI-like: resp.choices[0].message.content or resp.choices[0].text
        if hasattr(resp, "choices"):
            c = resp.choices
            if isinstance(c, (list, tuple)) and len(c):
                first = c[0]
                if hasattr(first, "message"):
                    if hasattr(first.message, "get"):
                        content = first.message.get("content", "")
                    elif hasattr(first.message, "content"):
                        content = first.message.content
                    else:
                        content = str(first.message)
                    logger.debug("[SCOPE] LLM ì‘ë‹µ ì¶”ì¶œ: choices[0].message")
                    return content
                if hasattr(first, "text"):
                    content = getattr(first, "text", "")
                    logger.debug("[SCOPE] LLM ì‘ë‹µ ì¶”ì¶œ: choices[0].text")
                    return content
        
        # some SDKs use .content directly
        if hasattr(resp, "content"):
            content = getattr(resp, "content")
            logger.debug("[SCOPE] LLM ì‘ë‹µ ì¶”ì¶œ: .content ì†ì„±")
            return content if isinstance(content, str) else str(content)
        
        # fallback to string conversion
        result = str(resp)
        logger.debug("[SCOPE] LLM ì‘ë‹µ ì¶”ì¶œ: str() ë³€í™˜")
        return result
    except Exception as e:
        logger.warning("[SCOPE] raw extract failed: %s", e)
        return str(resp) if resp else ""

# pipeline.pyì˜ _json_from_text í•¨ìˆ˜ êµì²´ (Line 156-170) #1107
def _json_from_text(maybe: str) -> Optional[dict]:
    """ë¬¸ìì—´ì—ì„œ JSON ì¶”ì¶œ (Markdown ì½”ë“œ ë¸”ë¡ ì§€ì›)"""
    if not maybe:
        return None
    
    try:
        s = maybe.strip()
        
        # â­ Markdown ì½”ë“œ ë¸”ë¡ ì œê±°
        # ```json\n{...}\n``` â†’ {...}
        s = re.sub(r'```json\s*', '', s)
        s = re.sub(r'```\s*', '', s)
        s = s.strip()
        
        # JSON íŒŒì‹±
        if s.startswith("{") and s.endswith("}"):
            result = json.loads(s)
            req_count = len(result.get("requirements", []))
            logger.info(f"âœ… [SCOPE] JSON íŒŒì‹± ì„±ê³µ (requirements={req_count})")
            return result
        
        # ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œ
        m = re.search(r"(\{[\s\S]*\})", s)
        if m:
            result = json.loads(m.group(1))
            req_count = len(result.get("requirements", []))
            logger.info(f"âœ… [SCOPE] ì •ê·œì‹ ì¶”ì¶œ ì„±ê³µ (requirements={req_count})")
            return result
            
    except json.JSONDecodeError as e:
        logger.error(f"[SCOPE] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        logger.error(f"[SCOPE] ì‘ë‹µ ì²˜ìŒ 500ì:\n{maybe[:500]}")
    except Exception as e:
        logger.error(f"[SCOPE] ì˜ˆì™¸: {e}")
    
    return None

# ============================================================================
# 1. _estimate_confidence í•¨ìˆ˜ ìˆ˜ì • (Line 172-202) #1107 confidence ë¬´ì‹œí•˜ê³  íŒŒì‹±
# ============================================================================

def _estimate_confidence(resp_json: Optional[dict], raw_text: str) -> float:
    """
    ê°œì„ ëœ confidence ì¶”ì •ê¸°:
    - ìš”êµ¬ì‚¬í•­ì´ ì—†ìœ¼ë©´ ë§¤ìš° ë‚®ì€ ì ìˆ˜ (0.1)
    - ìš”êµ¬ì‚¬í•­ ìˆ˜ì™€ í•„ë“œ ì™„ì „ì„±ì„ ëª¨ë‘ ê³ ë ¤
    - acceptance_criteria ì¡´ì¬ ì—¬ë¶€ë„ ì²´í¬
    """
    if resp_json and isinstance(resp_json, dict):
        # direct provided confidence
        if "confidence" in resp_json:
            try:
                c = float(resp_json["confidence"])
                return min(max(c, 0.0), 1.0)
            except Exception:
                pass
        
        # heuristic: requirements ìˆ˜ì™€ í’ˆì§ˆ
        reqs = resp_json.get("requirements")
        if not reqs or not isinstance(reqs, list):
            logger.debug("[SCOPE] confidence: requirementsê°€ ì—†ê±°ë‚˜ listê°€ ì•„ë‹˜")
            return 0.1  # ìš”êµ¬ì‚¬í•­ì´ ì—†ìœ¼ë©´ ë§¤ìš° ë‚®ì€ ì ìˆ˜
        
        if len(reqs) == 0:
            logger.debug("[SCOPE] confidence: requirements ë°°ì—´ì´ ë¹„ì–´ìˆìŒ")
            return 0.1  # ë¹ˆ ë°°ì—´ë„ ë§¤ìš° ë‚®ì€ ì ìˆ˜
        
        # ìš”êµ¬ì‚¬í•­ ìˆ˜ì— ë”°ë¥¸ ê¸°ë³¸ ì ìˆ˜
        if len(reqs) < 3:
            base_score = 0.3  # ë„ˆë¬´ ì ìŒ
        elif len(reqs) < 5:
            base_score = 0.5  # ì ìŒ
        elif len(reqs) < 10:
            base_score = 0.6  # ë³´í†µ
        else:
            base_score = 0.7  # ì¶©ë¶„
        
        # í•„ë“œ ì™„ì „ì„± ì²´í¬
        filled = 0
        has_ac = 0  # acceptance_criteria ìˆëŠ” ê²ƒ
        
        for r in reqs:
            # í•„ìˆ˜ í•„ë“œ
            has_required = (
                r.get("req_id") and 
                r.get("title") and 
                r.get("description") and
                r.get("type") and
                r.get("priority")
            )
            
            if has_required:
                filled += 1
            
            # acceptance_criteria ì²´í¬
            ac = r.get("acceptance_criteria")
            if ac and isinstance(ac, list) and len(ac) >= 2:
                has_ac += 1
        
        if len(reqs) == 0:
            return 0.1
        
        field_ratio = filled / len(reqs)  # í•„ìˆ˜ í•„ë“œ ì¶©ì¡±ë¥ 
        ac_ratio = has_ac / len(reqs)     # acceptance_criteria ì¶©ì¡±ë¥ 
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        # base_score (0.3-0.7) + field_ratio (0-0.2) + ac_ratio (0-0.1)
        final_score = base_score + (field_ratio * 0.2) + (ac_ratio * 0.1)
        
        logger.debug(
            f"[SCOPE] confidence: {len(reqs)}ê°œ ìš”êµ¬ì‚¬í•­, "
            f"í•„ë“œ ì¶©ì¡± {filled}/{len(reqs)}, "
            f"AC ì¶©ì¡± {has_ac}/{len(reqs)}, "
            f"ì ìˆ˜ {final_score:.3f}"
        )
        
        return min(final_score, 0.99)
    
    # fallback: JSON íŒŒì‹± ì‹¤íŒ¨
    logger.debug("[SCOPE] confidence: JSON íŒŒì‹± ì‹¤íŒ¨")
    return 0.1

def _ensure_req_ids(reqs: List[dict]) -> List[dict]:
    """req_idê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±"""
    out = []
    ts = int(time.time())
    counter = 0
    for r in reqs:
        if not r.get("req_id"):
            counter += 1
            r["req_id"] = f"REQ-{ts}-{counter:02}"
        out.append(r)
    return out

# ------- ScopeAgent -------

class ScopeAgent:
    """RFP ë¬¸ì„œë¡œë¶€í„° Requirements/SRS/RTM/WBS(ì´ˆì•ˆ) ë“±ì„ ìƒì„±í•˜ëŠ” Agent
       ì¶”ê°€ ì˜µì…˜ (payload['options']):
         - confidence_threshold: float (0..1), default=0.75
         - max_attempts: int, default=3
    """

    def __init__(self, data_dir: Optional[str] = None):
        self.llm = get_llm()
        self.data_dir = data_dir or "data"
        logger.info(f"[SCOPE_AGENT] ì´ˆê¸°í™” ì™„ë£Œ - data_dir: {self.data_dir}")

    async def pipeline(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        project_id = payload.get("project_id") or payload.get("project_name") or "Unknown"
        text = payload.get("text") or ""
        documents = payload.get("documents") or []
        options = payload.get("options") or {}
        confidence_threshold = float(options.get("confidence_threshold", 0.75))
        max_attempts = int(options.get("max_attempts", 3))

        # if no text but documents - read first file path if exists (best-effort)
        if not text and documents:
            first = documents[0]
            path = None
            if isinstance(first, dict):
                path = first.get("path")
            else:
                path = getattr(first, "path", None)
            if path:
                p = Path(path)
                if not p.exists():
                    # maybe relative to data/inputs/RFP
                    alt = Path("data/inputs/RFP") / Path(path).name
                    if alt.exists():
                        p = alt
                if p.exists():
                    try:
                        # simple read for txt; for docx/pdf we expect upper layer to convert
                        text = p.read_text(encoding="utf-8", errors="ignore")
                    except Exception:
                        text = ""

        logger.info("ğŸ”µ [SCOPE] ìš”ì²­: project_id=%s, methodology=%s", project_id, payload.get("methodology"))

        # Run extraction with confidence loop
        items, raw_resp = await self._extract_items_with_confidence(text, confidence_threshold, max_attempts)

        # Ensure req_ids
        reqs = items.get("requirements", [])
        if reqs:
            items["requirements"] = _ensure_req_ids(reqs)

        # Write outputs: srs, scope md, rtm csv, wbs json draft
        out_dir = Path("data/outputs/scope") / str(project_id)
        out_dir.mkdir(parents=True, exist_ok=True)

        srs_path = out_dir / f"{project_id}_SRS.md"
        self._generate_srs(project_id, items, srs_path)

        # WBS draft: keep simple hierarchical draft (could be improved via WBS synthesis step)
        wbs = await self._synthesize_wbs_draft(items, depth=int(options.get("wbs_depth", 3)))
        wbs_path = out_dir / "wbs_structure.json"
        wbs_path.write_text(json.dumps(wbs, ensure_ascii=False, indent=2), encoding="utf-8")

        # RTM csv
        rtm_csv = out_dir / "rtm.csv"
        with rtm_csv.open("w", encoding="utf-8", newline="") as fh:
            fh.write("req_id,wbs_id,test_case,verification_status\n")
            for r in items.get("requirements", []):
                fh.write(f"{r.get('req_id')},,,Candidate\n")

        # attempt DB save (best-effort)
        saved = 0
        if _DB_AVAILABLE:
            try:
                saved = self._save_requirements_db(project_id, items)
            except Exception as e:
                logger.exception("[SCOPE] DB save failed: %s", e)
                saved = 0
        else:
            logger.debug("[SCOPE] DB not available, skipping DB save")

        # PMP outputs (scope_statement excel etc.) - keep existing hooks
        pmp_outputs = self._generate_pmp_outputs(project_id, out_dir, items)

        result = {
            "status": "ok",
            "project_id": project_id,
            "requirements": items.get("requirements", []),
            "functions": items.get("functions", []),
            "wbs_json": str(wbs_path),
            "wbs": wbs,
            "rtm_csv": str(rtm_csv),
            "srs_path": str(srs_path),
            "pmp_outputs": pmp_outputs,
            "db_saved_requirements": saved,
            "_llm_raw_response": str(raw_resp)[:2000],
        }
        logger.info("âœ… [SCOPE] ì‘ë‹µì™„ë£Œ: %s (requirements=%d, saved=%d)", project_id, len(items.get("requirements", [])), saved)
        return result

# ============================================================================
# 2. _extract_items_with_confidence ë©”ì„œë“œ ìˆ˜ì • (Line 318-430) #1107 confidence ë¬´ì‹œí•˜ê³  íŒŒì‹±
# ============================================================================

    async def _extract_items_with_confidence(
        self, text: str, threshold: float = 0.75, max_attempts: int = 3
    ) -> tuple:
        """
        ê°œì„ ëœ confidence ê¸°ë°˜ ì¶”ì¶œ
        
        ë³€ê²½ì‚¬í•­:
        1. ë¹ˆ ê²°ê³¼ ì¡°ê¸° ê°ì§€ ë° ì¬ì‹œë„
        2. refinement prompt ê°œì„ 
        3. threshold ë™ì  ì¡°ì •
        """
        if not text:
            return {"requirements": [], "functions": []}, None

        llm = self.llm
        attempt = 0
        last_items = None
        last_raw = None
        
        # ì‹œë„ë³„ threshold ì™„í™” (ë„ˆë¬´ ì—„ê²©í•˜ë©´ ì˜ì›íˆ í†µê³¼ ëª»í•¨)
        attempt_thresholds = {
            1: threshold,           # 0.75
            2: threshold - 0.05,    # 0.70
            3: threshold - 0.10     # 0.65
        }

        while attempt < max_attempts:
            attempt += 1
            current_threshold = attempt_thresholds.get(attempt, threshold)
            
            logger.info(
                "ğŸ”µ [SCOPE] LLM ì‹œë„ #%d/%d (threshold=%.2f)", 
                attempt, max_attempts, current_threshold
            )

            # Build prompt
            if last_items is None:
                # ì²« ì‹œë„: ì¼ë°˜ ì¶”ì¶œ
                prompt = SCOPE_EXTRACT_PROMPT.format(context=text[:8000])
            else:
                # ì¬ì‹œë„: ê°œì„ ëœ refinement prompt
                prev_req_count = len(last_items.get("requirements", []))
                
                prompt = f"""
    ì´ì „ ì¶”ì¶œ ê²°ê³¼ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.

    **ë¬¸ì œì :**
    - ì¶”ì¶œëœ ìš”êµ¬ì‚¬í•­: {prev_req_count}ê°œ (ë„ˆë¬´ ì ìŒ)
    - ëˆ„ë½ëœ ì„¹ì…˜ì´ ìˆì„ ê°€ëŠ¥ì„±

    **ì´ì „ ê²°ê³¼:**
    {json.dumps(last_items, ensure_ascii=False, indent=2)[:2000]}

    **ì›ë¬¸ (ì²˜ìŒ 6000ì):**
    {text[:6000]}

    **ê°œì„  ìš”ì²­:**
    1. ë¬¸ì„œ ì „ì²´ë¥¼ ë‹¤ì‹œ ê²€í† í•˜ì—¬ ëˆ„ë½ëœ ìš”êµ¬ì‚¬í•­ ì°¾ê¸°
    2. ê° ì„¹ì…˜ì—ì„œ ìµœì†Œ 1-2ê°œ ì´ìƒì˜ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
    3. ìš”êµ¬ì‚¬í•­ì´ 10ê°œ ë¯¸ë§Œì´ë©´ ë” ì„¸ë¶„í™”
    4. ëª¨ë“  ìš”êµ¬ì‚¬í•­ì— ë‹¤ìŒ í•„ë“œ í¬í•¨:
    - req_id, title, description, type, priority, source_span
    - acceptance_criteria (ìµœì†Œ 2ê°œ)

    **ì¶œë ¥ í˜•ì‹ (JSONë§Œ):**
    {{{{
    "requirements": [
        {{{{
        "req_id": "REQ-001",
        "title": "...",
        "type": "functional",
        "priority": "High",
        "description": "...",
        "source_span": "...",
        "acceptance_criteria": ["...", "..."]
        }}}}
    ]
    }}}}
    """

            # LLM í˜¸ì¶œ
            raw_resp = None
            parsed = None
            try:
                if llm:
                    logger.info(f"ğŸ¤– [SCOPE] LLM í˜¸ì¶œ ì‹œì‘ #5(í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)})")
                    
                    def call():
                        try:
                            if hasattr(llm, "invoke"):
                                messages = [
                                    {"role": "system", "content": "You are a PM analyst assistant. Always return valid JSON."},
                                    {"role": "user", "content": prompt}
                                ]
                                return llm.invoke(messages)
                            elif hasattr(llm, "generate"):
                                return llm.generate(prompt)
                            elif callable(llm):
                                return llm(prompt)
                            return None
                        except Exception as e:
                            logger.error(f"[SCOPE] LLM í˜¸ì¶œ ì˜ˆì™¸: {e}")
                            raise

                    resp = await asyncio.to_thread(call)
                    logger.info(f"âœ… [SCOPE] LLM ì‘ë‹µ ìˆ˜ì‹  (íƒ€ì…: {type(resp).__name__})")
                    
                    raw_resp = _safe_extract_raw(resp)
                    logger.info(f"ğŸ“„ [SCOPE] ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ (ê¸¸ì´: {len(str(raw_resp))})")
                else:
                    logger.warning("[SCOPE] LLM ë¯¸ì„¤ì •, fallback ì‚¬ìš©")
                    return self._fallback_extract(text), None
                    
            except Exception as e:
                logger.warning("ğŸŸ  [SCOPE] LLM í˜¸ì¶œ ì‹¤íŒ¨: %s", e)
                if attempt == max_attempts:
                    return self._fallback_extract(text), None
                continue

            # JSON íŒŒì‹±
            parsed = _json_from_text(raw_resp)
            confidence = _estimate_confidence(parsed, raw_resp)
            
            if parsed:
                req_count = len(parsed.get("requirements", []))
                logger.info(
                    "ğŸ”µ [SCOPE] parsed=True, requirements=%d, confidence=%.3f",
                    req_count, confidence
                )
            else:
                logger.warning("ğŸŸ  [SCOPE] parsed=False (JSON íŒŒì‹± ì‹¤íŒ¨)")

            # Confidence ì²´í¬
            if parsed and confidence >= current_threshold:
                req_count = len(parsed.get("requirements", []))
                logger.info(
                    "âœ… [SCOPE] í†µê³¼! (confidence %.3f >= %.3f, requirements=%d) on attempt %d",
                    confidence, current_threshold, req_count, attempt
                )
                return parsed, raw_resp

            # ì¬ì‹œë„ ê²°ì •
            if parsed:
                req_count = len(parsed.get("requirements", []))
                
                # ìš”êµ¬ì‚¬í•­ì´ 0ê°œë©´ ì¦‰ì‹œ ì¬ì‹œë„
                if req_count == 0:
                    logger.warning(
                        "âš ï¸ [SCOPE] requirements=0ê°œ. ì¦‰ì‹œ ì¬ì‹œë„ (attempt %d/%d)",
                        attempt, max_attempts
                    )
                    last_items = None  # ì´ì „ ê²°ê³¼ ë¬´ì‹œ
                    await asyncio.sleep(0.2)
                    continue
                
                # ìš”êµ¬ì‚¬í•­ì´ ìˆì§€ë§Œ confidence ë‚®ìŒ
                last_items = parsed
                last_raw = raw_resp
                logger.info(
                    "[SCOPE] ì¬ì‹œë„: confidence %.3f < %.3f (requirements=%d)",
                    confidence, current_threshold, req_count
                )
                await asyncio.sleep(0.2)
            else:
                # íŒŒì‹± ì‹¤íŒ¨
                logger.warning("[SCOPE] ì¬ì‹œë„: JSON íŒŒì‹± ì‹¤íŒ¨")
                last_items = None
                last_raw = raw_resp
                await asyncio.sleep(0.2)

        # ìµœëŒ€ ì‹œë„ ë„ë‹¬
        if last_items and len(last_items.get("requirements", [])) > 0:
            req_count = len(last_items.get("requirements", []))
            logger.warning(
                "[SCOPE] ìµœëŒ€ ì‹œë„(%d) ë„ë‹¬: ë§ˆì§€ë§‰ ê²°ê³¼ ì‚¬ìš© (requirements=%d, confidence %.3f)",
                max_attempts, req_count, confidence
            )
            return last_items, last_raw
        else:
            logger.error("[SCOPE] ìµœëŒ€ ì‹œë„(%d) ë„ë‹¬: ìœ íš¨í•œ ê²°ê³¼ ì—†ìŒ. fallback ì‚¬ìš©", max_attempts)
            return self._fallback_extract(text), None


    def _fallback_extract(self, text: str) -> Dict[str, Any]:
        """ê°„ë‹¨ ê·œì¹™ ê¸°ë°˜ (ê¸°ì¡´ fallback ìœ ì§€)"""
        reqs = []
        funcs = []
        for i, ln in enumerate([l.strip() for l in text.splitlines() if l.strip()]):
            if re.search(r"(ìš”êµ¬|í•„ìš”|í•´ì•¼|shall|must|should)", ln, re.I):
                reqs.append({
                    "req_id": None,
                    "title": ln[:80],
                    "type": "functional",
                    "priority": "Medium",
                    "description": ln,
                    "source_span": f"line {i+1}"
                })
            if re.search(r"(ê¸°ëŠ¥|ê¸°ëŠ¥ëª…|support|provide)", ln, re.I):
                funcs.append({"name": ln[:80], "desc": ln})
        logger.info("âœ… [SCOPE] fallback ì¶”ì¶œ: %d reqs, %d funcs", len(reqs), len(funcs))
        return {"requirements": reqs, "functions": funcs}

    async def _synthesize_wbs_draft(self, items: Dict[str, Any], depth: int = 3) -> Dict[str, Any]:
        """ê°„ë‹¨í•œ WBS ì´ˆì•ˆ ìƒì„± (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        nodes = [{
            "id": "WBS-1",
            "name": "Project",
            "level": 1,
            "children": []
        }]
        phases = []
        reqs = items.get("requirements", [])
        # naive: split into phases of ~ceil(len(reqs)/3)
        if reqs:
            per = max(1, (len(reqs) + 2) // 3)
            for i in range(3):
                start = i * per
                seg = reqs[start:start+per]
                phase = {
                    "id": f"WBS-1.{i+1}",
                    "name": f"Phase {i+1}",
                    "level": 2,
                    "children": []
                }
                # tasks per requirement
                for j, r in enumerate(seg, 1):
                    phase["children"].append({
                        "id": f"{phase['id']}.{j}",
                        "name": r.get("title", f"Task {i+1}.{j}")[:60],
                        "level": 3,
                        "owner": None,
                        "deliverables": None
                    })
                phases.append(phase)
        else:
            # default phases
            for i in range(3):
                phases.append({
                    "id": f"WBS-1.{i+1}",
                    "name": f"Phase {i+1}",
                    "level": 2,
                    "children": []
                })
        nodes[0]["children"] = phases
        return {"nodes": nodes, "depth": depth}

    def _save_requirements_db(self, project_id: str, items: Dict[str, Any]) -> int:
        """
        DBì— ìš”êµ¬ì‚¬í•­ ì €ì¥(ê°„ë‹¨ êµ¬í˜„). ë°˜í™˜: ì €ì¥ëœ ë ˆì½”ë“œ ìˆ˜.
        ì•ˆì „ ì¥ì¹˜: req_idê°€ ë¹„ì–´ìˆìœ¼ë©´ ìë™ìƒì„± í›„ ì €ì¥.
        """
        if not _DB_AVAILABLE:
            logger.debug("[SCOPE] DB not available")
            return 0
        db = SessionLocal()
        saved = 0
        try:
            reqs = items.get("requirements", []) or []
            for r in reqs:
                req_id = r.get("req_id")
                title = r.get("title") or r.get("description")[:200]
                description = r.get("description")
                rtype = r.get("type") or r.get("category") or "functional"
                priority = r.get("priority") or "Medium"
                source_doc = r.get("source_span") or None

                if not req_id:
                    req_id = f"AUTO-{int(time.time())}-{saved+1}"
                    logger.debug("[SCOPE] ìë™ req_id ìƒì„±: %s", req_id)

                # upsert-like: try find existing by project_id + req_id
                existing = db.query(pm_models.PM_Requirement).filter_by(project_id=project_id, req_id=req_id).first()
                if existing:
                    existing.title = title
                    existing.description = description
                    existing.priority = priority
                    existing.type = rtype
                    existing.source_doc = source_doc
                else:
                    obj = pm_models.PM_Requirement(
                        project_id=project_id,
                        req_id=req_id,
                        title=title,
                        description=description,
                        priority=priority,
                        status="new",
                        source_doc=source_doc,
                        created_at=datetime.utcnow()
                    )
                    db.add(obj)
                saved += 1
            db.commit()
            logger.info("[SCOPE] DB ì €ì¥ ì™„ë£Œ: %d ë ˆì½”ë“œ", saved)
            return saved
        except Exception as e:
            logger.exception("[SCOPE] Saving to DB failed: %s", e)
            try:
                db.rollback()
            except Exception:
                pass
            return 0
        finally:
            db.close()

    # SRS / Charter / PMP outputs (existing hooks)
    def _generate_srs(self, project_id: Any, items: Dict[str, Any], out_path: Path):
        out_path.parent.mkdir(parents=True, exist_ok=True)
        with open(out_path, "w", encoding="utf-8") as f:
            f.write(f"# Software Requirements Specification\n")
            f.write(f"**Project:** {project_id}\n")
            f.write(f"**Generated:** {datetime.utcnow().isoformat()}\n\n")
            f.write("## 1. Requirements\n\n")
            for r in items.get("requirements", []):
                f.write(f"### {r.get('req_id')}: {r.get('title')}\n")
                f.write(f"- **Type:** {r.get('type')}\n")
                f.write(f"- **Priority:** {r.get('priority')}\n")
                f.write(f"- **Description:** {r.get('description')}\n")
                f.write(f"- **Source:** {r.get('source_span')}\n\n")
        return str(out_path)

    def _generate_pmp_outputs(self, project_id: Any, project_dir: Path, requirements: Dict[str, Any]) -> Dict[str, Optional[str]]:
        outputs = {}
        try:
            from .outputs.scope_statement import ScopeStatementGenerator
            scp = project_dir / f"{project_id}_ScopeStatement.xlsx"
            outputs["scope_statement_excel"] = ScopeStatementGenerator.generate(project_id, requirements, scp)
        except Exception as e:
            outputs["scope_statement_excel"] = None
            logger.debug("ScopeStatementGenerator not available: %s", e)
        return outputs
    
    # ScopeAgentì— ì¶”ê°€í•  ë©”ì„œë“œë“¤
    # pipeline.pyì˜ ScopeAgent í´ë˜ìŠ¤ì— ì¶”ê°€

    def refine_requirements(self, 
                        text: str, 
                        previous_result: List[Dict[str, Any]],
                        validation_result: Dict[str, Any],
                        project_meta: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        ê²€ì¦ í”¼ë“œë°±ì„ ë°˜ì˜í•œ ìš”êµ¬ì‚¬í•­ ì¬ì¶”ì¶œ
        
        Args:
            text: ì›ë³¸ ë¬¸ì„œ
            previous_result: ì´ì „ ì¶”ì¶œ ê²°ê³¼
            validation_result: í’ˆì§ˆ ê²€ì¦ ê²°ê³¼
            project_meta: í”„ë¡œì íŠ¸ ë©”íƒ€ë°ì´í„°
        
        Returns:
            ê°œì„ ëœ ìš”êµ¬ì‚¬í•­ ë¦¬ìŠ¤íŠ¸
        """
        logger.info("[SCOPE] í”¼ë“œë°± ê¸°ë°˜ ì¬ì¶”ì¶œ ì‹œì‘")
        
        # ê²€ì¦ ê²°ê³¼ ë¶„ì„
        score = validation_result.get('score', 0)
        issues = validation_result.get('issues', [])
        missing = validation_result.get('missing_requirements', [])
        recommendations = validation_result.get('recommendations', [])
        
        logger.info(f"[SCOPE] ì´ì „ ì ìˆ˜: {score}")
        logger.info(f"[SCOPE] ì´ìŠˆ: {len(issues)}ê°œ")
        logger.info(f"[SCOPE] ëˆ„ë½: {len(missing)}ê°œ")
        
        # í”¼ë“œë°± í”„ë¡¬í”„íŠ¸ ìƒì„±
        feedback_section = self._build_feedback_section(
            score, issues, missing, recommendations
        )
        
        # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
        refinement_prompt = f"""
    ë‹¹ì‹ ì€ PMP í‘œì¤€ì„ ì¤€ìˆ˜í•˜ëŠ” ì „ë¬¸ PMO ë¶„ì„ê°€ì…ë‹ˆë‹¤.

    ## ğŸ”„ ìš”êµ¬ì‚¬í•­ ê°œì„  ì‘ì—…

    ### ì´ì „ ì¶”ì¶œ ê²°ê³¼
    ì¶”ì¶œëœ ìš”êµ¬ì‚¬í•­: {len(previous_result)}ê°œ
    í’ˆì§ˆ ì ìˆ˜: {score}/100

    ### ğŸ“‹ ì´ì „ ì¶”ì¶œ ê²°ê³¼ (ìš”ì•½)
    {self._summarize_requirements(previous_result)}

    ### âš ï¸ ê²€ì¦ì—ì„œ ë°œê²¬ëœ ë¬¸ì œì 

    {feedback_section}

    ### ğŸ¯ ê°œì„  ì§€ì¹¨

    1. **ëˆ„ë½ëœ ìš”êµ¬ì‚¬í•­ ì¶”ê°€**
    {self._format_missing(missing)}

    2. **ê¸°ì¡´ ìš”êµ¬ì‚¬í•­ ê°œì„ **
    {self._format_issues(issues)}

    3. **ê¶Œì¥ì‚¬í•­ ë°˜ì˜**
    {self._format_recommendations(recommendations)}

    ### ğŸ“ ê°œì„  ì›ì¹™

    - ëª¨í˜¸í•œ í‘œí˜„ ì œê±°: "ì ì ˆíˆ", "ì¶©ë¶„íˆ" â†’ êµ¬ì²´ì  ê¸°ì¤€
    - ì¸¡ì • ê°€ëŠ¥ì„±: ëª¨ë“  ìš”êµ¬ì‚¬í•­ì— ì •ëŸ‰ì  ê¸°ì¤€ í¬í•¨
    - acceptance_criteria: ìµœì†Œ 3ê°œ ì´ìƒ, ê°ê° ê²€ì¦ ê°€ëŠ¥
    - ì„¸ë¶„í™”: í•˜ë‚˜ì˜ ìš”êµ¬ì‚¬í•­ = í•˜ë‚˜ì˜ ê¸°ëŠ¥

    ---

    ## ğŸ“„ ì›ë¬¸ ë¬¸ì„œ
    {text[:6000]}

    ---

    ìœ„ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ìš”êµ¬ì‚¬í•­ì„ ê°œì„ í•˜ì„¸ìš”.
    ê¸°ì¡´ ìš”êµ¬ì‚¬í•­ì€ ìœ ì§€í•˜ë˜, ë¬¸ì œê°€ ìˆëŠ” ë¶€ë¶„ì€ ìˆ˜ì •í•˜ê³ , ëˆ„ë½ëœ ë¶€ë¶„ì€ ì¶”ê°€í•˜ì„¸ìš”.

    ì¶œë ¥ JSON:
    {{{{
    "requirements": [
        {{{{
        "req_id": "REQ-001",
        "title": "...",
        "type": "functional",
        "priority": "High",
        "description": "...",
        "source_span": "...",
        "acceptance_criteria": [...]
        }}}}
    ]
    }}}}
    """
        
        # LLM í˜¸ì¶œ
        try:
            messages = [
                {"role": "system", "content": "You are a PM analyst expert in requirements refinement."},
                {"role": "user", "content": refinement_prompt}
            ]
            
            logger.info("[SCOPE] LLM ì¬ì¶”ì¶œ í˜¸ì¶œ")
            resp = self.llm.invoke(messages)
            content = _safe_extract_raw(resp)
            
            logger.info(f"[SCOPE] ì‘ë‹µ ê¸¸ì´: {len(content)}")
            
            # JSON íŒŒì‹±
            json_text = _json_first(content)
            if not json_text:
                logger.warning("[SCOPE] JSON ì¶”ì¶œ ì‹¤íŒ¨, ì´ì „ ê²°ê³¼ ë°˜í™˜")
                return previous_result
            
            result = _postprocess(json_text, text)
            
            logger.info(f"[SCOPE] ì¬ì¶”ì¶œ ì™„ë£Œ: {len(result)}ê°œ ìš”êµ¬ì‚¬í•­")
            
            return result
            
        except Exception as e:
            logger.error(f"[SCOPE] ì¬ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return previous_result


    def _build_feedback_section(self, score, issues, missing, recommendations):
        """í”¼ë“œë°± ì„¹ì…˜ ìƒì„±"""
        
        sections = []
        
        # ì ìˆ˜ ë° ë“±ê¸‰
        if score < 60:
            grade = "Poor"
            emoji = "âŒ"
        elif score < 75:
            grade = "Fair"
            emoji = "âš ï¸"
        elif score < 90:
            grade = "Good"
            emoji = "âœ…"
        else:
            grade = "Excellent"
            emoji = "ğŸŒŸ"
        
        sections.append(f"{emoji} í’ˆì§ˆ ë“±ê¸‰: {grade} ({score}/100)")
        
        # ì´ìŠˆ
        if issues:
            sections.append(f"\n**ë°œê²¬ëœ ì´ìŠˆ ({len(issues)}ê°œ):**")
            for i, issue in enumerate(issues[:10], 1):
                sections.append(f"{i}. {issue}")
        
        # ëˆ„ë½
        if missing:
            sections.append(f"\n**ëˆ„ë½ëœ ìš”êµ¬ì‚¬í•­ ({len(missing)}ê°œ):**")
            for i, miss in enumerate(missing[:5], 1):
                sections.append(f"{i}. {miss}")
        
        # ê¶Œì¥ì‚¬í•­
        if recommendations:
            sections.append(f"\n**ê°œì„  ê¶Œì¥ì‚¬í•­ ({len(recommendations)}ê°œ):**")
            for i, rec in enumerate(recommendations[:5], 1):
                sections.append(f"{i}. {rec}")
        
        return "\n".join(sections)


    def _summarize_requirements(self, requirements):
        """ìš”êµ¬ì‚¬í•­ ìš”ì•½"""
        
        if not requirements:
            return "ì—†ìŒ"
        
        summary = []
        for req in requirements[:10]:
            summary.append(
                f"- {req.get('req_id')}: {req.get('title')} "
                f"({req.get('type')}, {req.get('priority')})"
            )
        
        if len(requirements) > 10:
            summary.append(f"... ì™¸ {len(requirements) - 10}ê°œ")
        
        return "\n".join(summary)


    def _format_missing(self, missing):
        """ëˆ„ë½ í•­ëª© í¬ë§·íŒ…"""
        
        if not missing:
            return "ì—†ìŒ"
        
        formatted = []
        for i, miss in enumerate(missing, 1):
            formatted.append(f"   {i}. {miss}")
        
        return "\n".join(formatted)


    def _format_issues(self, issues):
        """ì´ìŠˆ í¬ë§·íŒ…"""
        
        if not issues:
            return "ì—†ìŒ"
        
        formatted = []
        for i, issue in enumerate(issues[:10], 1):
            formatted.append(f"   {i}. {issue}")
        
        return "\n".join(formatted)


    def _format_recommendations(self, recommendations):
        """ê¶Œì¥ì‚¬í•­ í¬ë§·íŒ…"""
        
        if not recommendations:
            return "ì—†ìŒ"
        
        formatted = []
        for i, rec in enumerate(recommendations[:5], 1):
            formatted.append(f"   {i}. {rec}")
        
        return "\n".join(formatted)


    # ============================================================================
    # extract_with_validation í•¨ìˆ˜ ê°œì„  ë²„ì „
    # ============================================================================

    def extract_with_validation_v2(scope_agent, 
                                quality_agent,
                                text: str, 
                                project_meta: Optional[Dict] = None,
                                max_attempts: int = 3,
                                strategy: str = "auto") -> Dict[str, Any]:
        """
        ê²€ì¦ ê¸°ë°˜ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ (ê°œì„  ë²„ì „)
        
        Args:
            scope_agent: ScopeAgent ì¸ìŠ¤í„´ìŠ¤
            quality_agent: QualityAgent ì¸ìŠ¤í„´ìŠ¤
            text: ì›ë³¸ ë¬¸ì„œ
            project_meta: í”„ë¡œì íŠ¸ ë©”íƒ€ë°ì´í„°
            max_attempts: ìµœëŒ€ ì‹œë„ íšŸìˆ˜
            strategy: ì¬ì¶”ì¶œ ì „ëµ ("auto", "feedback", "staged", "examples")
        
        Returns:
            {
                "success": bool,
                "requirements": [...],
                "validation": {...},
                "attempts": int,
                "improvements": [...],
                "history": [...]
            }
        """
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸš€ ê²€ì¦ ê¸°ë°˜ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ (ê°œì„  ë²„ì „)")
        logger.info(f"ğŸ“ ë¬¸ì„œ ê¸¸ì´: {len(text)} ë¬¸ì")
        logger.info(f"ğŸ”„ ìµœëŒ€ ì‹œë„: {max_attempts}íšŒ")
        logger.info(f"ğŸ“‹ ì „ëµ: {strategy}")
        logger.info(f"{'='*70}\n")
        
        history = []
        improvements = []
        previous_result = None
        validation_result = None
        
        for attempt in range(1, max_attempts + 1):
            logger.info(f"\n{'â”€'*70}")
            logger.info(f"ğŸ“ ì‹œë„ #{attempt}/{max_attempts}")
            logger.info(f"{'â”€'*70}")
            
            # 1. ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ
            if attempt == 1:
                # ì²« ì‹œë„: ì¼ë°˜ ì¶”ì¶œ
                logger.info("ğŸ” ì´ˆê¸° ì¶”ì¶œ ìˆ˜í–‰")
                result = scope_agent.analyze_rfp(text, project_meta)
                method = "initial"
            else:
                # ì¬ì‹œë„: í”¼ë“œë°± ë°˜ì˜ ì¬ì¶”ì¶œ
                logger.info(f"ğŸ”„ í”¼ë“œë°± ê¸°ë°˜ ì¬ì¶”ì¶œ ìˆ˜í–‰")
                logger.info(f"   ì´ì „ ì ìˆ˜: {validation_result['score']:.1f}")
                logger.info(f"   ì´ìŠˆ: {len(validation_result['issues'])}ê°œ")
                logger.info(f"   ëˆ„ë½: {len(validation_result['missing_requirements'])}ê°œ")
                
                # refine_requirements ë©”ì„œë“œ í˜¸ì¶œ
                result = scope_agent.refine_requirements(
                    text, 
                    previous_result, 
                    validation_result,
                    project_meta
                )
                method = "refined"
            
            # ê²°ê³¼ ì •ê·œí™”
            requirements = result if isinstance(result, list) else result.get('requirements', [])
            logger.info(f"âœ… {len(requirements)}ê°œ ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ ì™„ë£Œ")
            
            # 2. í’ˆì§ˆ ê²€ì¦
            logger.info("ğŸ” í’ˆì§ˆ ê²€ì¦ ì‹œì‘")
            validation_result = quality_agent.validate(requirements, text, project_meta)
            
            current_score = validation_result['score']
            logger.info(f"ğŸ¯ ì ìˆ˜: {current_score:.1f} ({validation_result['grade']})")
            
            # ê°œì„ ë„ ê³„ì‚°
            if previous_result:
                prev_score = history[-1]['validation']['score']
                improvement = current_score - prev_score
                improvements.append(improvement)
                logger.info(f"ğŸ“ˆ ê°œì„ ë„: {improvement:+.1f}ì ")
            
            # íˆìŠ¤í† ë¦¬ ì €ì¥
            history.append({
                "attempt": attempt,
                "method": method,
                "requirements_count": len(requirements),
                "validation": validation_result,
                "timestamp": datetime.now().isoformat()
            })
            
            # 3. í†µê³¼ ì—¬ë¶€ í™•ì¸
            if validation_result['pass']:
                logger.info(f"\n{'='*70}")
                logger.info(f"âœ… ê²€ì¦ í†µê³¼! (ì‹œë„ {attempt}íšŒ)")
                logger.info(f"ğŸ¯ ìµœì¢… ì ìˆ˜: {current_score:.1f}")
                logger.info(f"ğŸ† ë“±ê¸‰: {validation_result['grade']}")
                
                if attempt > 1:
                    total_improvement = current_score - history[0]['validation']['score']
                    logger.info(f"ğŸ“ˆ ì´ ê°œì„ : {total_improvement:+.1f}ì ")
                
                logger.info(f"{'='*70}\n")
                
                return {
                    "success": True,
                    "requirements": requirements,
                    "validation": validation_result,
                    "attempts": attempt,
                    "method": method,
                    "improvements": improvements,
                    "history": history,
                    "message": f"ê²€ì¦ í†µê³¼ (ì‹œë„ {attempt}íšŒ, ìµœì¢… ì ìˆ˜ {current_score:.1f})"
                }
            
            # ê²€ì¦ ì‹¤íŒ¨
            logger.warning(f"\nâš ï¸ ê²€ì¦ ë¯¸í†µê³¼")
            logger.warning(f"   ì ìˆ˜: {current_score:.1f} (ê¸°ì¤€: {quality_agent.threshold})")
            logger.warning(f"   ë“±ê¸‰: {validation_result['grade']}")
            
            if validation_result['issues']:
                logger.warning(f"\nğŸ“‹ ì£¼ìš” ì´ìŠˆ:")
                for i, issue in enumerate(validation_result['issues'][:5], 1):
                    logger.warning(f"   {i}. {issue}")
            
            if validation_result['missing_requirements']:
                logger.warning(f"\nğŸ“‹ ëˆ„ë½ í•­ëª©:")
                for i, miss in enumerate(validation_result['missing_requirements'][:3], 1):
                    logger.warning(f"   {i}. {miss}")
            
            # ë§ˆì§€ë§‰ ì‹œë„ ì „ ê²½ê³ 
            if attempt == max_attempts - 1:
                logger.warning(f"\nâš ï¸ ë§ˆì§€ë§‰ ì‹œë„ ì „ì…ë‹ˆë‹¤!")
            
            previous_result = requirements
        
        # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
        final_score = validation_result['score']
        logger.error(f"\n{'='*70}")
        logger.error(f"âŒ {max_attempts}íšŒ ì‹œë„ í›„ì—ë„ ê²€ì¦ ê¸°ì¤€ ë¯¸ë‹¬")
        logger.error(f"   ìµœì¢… ì ìˆ˜: {final_score:.1f} (ê¸°ì¤€: {quality_agent.threshold})")
        logger.error(f"   ìµœì¢… ë“±ê¸‰: {validation_result['grade']}")
        
        if improvements:
            avg_improvement = sum(improvements) / len(improvements)
            logger.error(f"   í‰ê·  ê°œì„ : {avg_improvement:+.1f}ì /ì‹œë„")
        
        logger.error(f"{'='*70}\n")
        
        return {
            "success": False,
            "requirements": requirements,
            "validation": validation_result,
            "attempts": max_attempts,
            "method": method,
            "improvements": improvements,
            "history": history,
            "message": f"í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ (ìµœì¢… {final_score:.1f}ì )"
        }
